{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for model development scratchwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word      prob\n",
      "0      The -1.192179\n",
      "1      Ġir -3.641227\n",
      "2      ish -2.003065\n",
      "3    Ġlike -3.312317\n",
      "4     Ġall -1.335042\n",
      "5  Ġthings -0.902359\n",
      "6  Ġexcept -3.136899\n",
      "7    Ġheat -3.599497\n",
      "8     </s> -4.674141\n",
      "new\n",
      "      word      prob\n",
      "0      The -1.192179\n",
      "1      Ġir -3.641227\n",
      "2      ish -2.003065\n",
      "3    Ġlike -3.312317\n",
      "4     Ġall -1.335042\n",
      "5  Ġthings -0.902359\n",
      "6  Ġexcept -3.136899\n",
      "7    Ġheat -3.599497\n",
      "     word      prob\n",
      "0     the -0.036050\n",
      "1   irish -3.715711\n",
      "2    like -1.567202\n",
      "3     all -0.773892\n",
      "4  things -0.512565\n",
      "5  except -1.972257\n",
      "6    heat -3.555819\n",
      "7   [SEP] -6.056349\n",
      "new\n",
      "     word      prob\n",
      "0     the -0.036050\n",
      "1   irish -3.715711\n",
      "2    like -1.567202\n",
      "3     all -0.773892\n",
      "4  things -0.512565\n",
      "5  except -1.972257\n",
      "6    heat -3.555819\n",
      "            word      prob\n",
      "0            The -1.423661\n",
      "1            Ġir -4.702773\n",
      "2            ish -1.480454\n",
      "3          Ġlike -3.679087\n",
      "4           Ġall -2.775838\n",
      "5        Ġthings -2.307309\n",
      "6        Ġexcept -3.281427\n",
      "7          Ġheat -3.063829\n",
      "8  <|endoftext|> -2.548836\n",
      "new\n",
      "      word      prob\n",
      "0      The -1.423661\n",
      "1      Ġir -4.702773\n",
      "2      ish -1.480454\n",
      "3    Ġlike -3.679087\n",
      "4     Ġall -2.775838\n",
      "5  Ġthings -2.307309\n",
      "6  Ġexcept -3.281427\n",
      "7    Ġheat -3.063829\n",
      "            word      prob\n",
      "0            The -0.921320\n",
      "1            Ġir -4.623364\n",
      "2            ish -1.875706\n",
      "3          Ġlike -4.054720\n",
      "4           Ġall -2.567827\n",
      "5        Ġthings -1.303209\n",
      "6        Ġexcept -3.905859\n",
      "7          Ġheat -3.770711\n",
      "8  <|endoftext|> -2.437582\n",
      "new\n",
      "      word      prob\n",
      "0      The -0.921320\n",
      "1      Ġir -4.623364\n",
      "2      ish -1.875706\n",
      "3    Ġlike -4.054720\n",
      "4     Ġall -2.567827\n",
      "5  Ġthings -1.303209\n",
      "6  Ġexcept -3.905859\n",
      "7    Ġheat -3.770711\n"
     ]
    }
   ],
   "source": [
    "## Checking correctness for filter_eos_df\n",
    "\n",
    "from new_models import prep_probs\n",
    "\n",
    "\n",
    "model_names = ['bart', 'bert', 'gpt2_normal', 'gpt2_medium']\n",
    "\n",
    "RESULTS_FOLDER = './intermediate_results/new_models_probs'\n",
    "\n",
    "for model_name in model_names:\n",
    "    results_path = join(RESULTS_FOLDER, f'{model_name}_predictions.txt')\n",
    "    this_df = prep_probs.load_word_scores(model_name, RESULTS_FOLDER)[54]\n",
    "    \n",
    "    print(this_df)\n",
    "    \n",
    "    new_df = prep_probs.filter_eos_df(this_df)\n",
    "    \n",
    "    print('new')\n",
    "    print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# 3/27: https://huggingface.co/transformers/model_doc/bart.html\n",
    "sentence = \"It's time to go to the <mask>\"\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "batch = tokenizer(sentence, return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<s>It's time to go to the<mask></s>\"]\n"
     ]
    }
   ],
   "source": [
    "#3/27: https://huggingface.co/transformers/model_doc/bart.html\n",
    "\n",
    "decoded_batch = tokenizer.batch_decode(batch['input_ids'])\n",
    "\n",
    "# 3/27: LM reference https://huggingface.co/transformers/model_doc/bart.html#barttokenizer\n",
    "this_lm = model.generate(batch['input_ids'])\n",
    "    \n",
    "print(decoded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(this_lm) # How to extract the softmax itself?\n",
    "result = model.forward(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50264])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<mask>']"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_int_mask = batch['input_ids'][0][-2:-1]\n",
    "print(bart_int_mask)\n",
    "tokenizer.batch_decode(bart_int_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 50265])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word          prob\n",
      "0         each  4.720370e-01\n",
      "1   nonfiction  1.034481e-04\n",
      "2         book  6.232377e-03\n",
      "3          has  8.093700e-01\n",
      "4            a  9.429873e-01\n",
      "5         call  2.517068e-03\n",
      "6       number  2.460797e-02\n",
      "7           on  9.372336e-01\n",
      "8          its  3.873985e-01\n",
      "9        spine  9.837382e-03\n",
      "10       [SEP]  7.783939e-07\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join, exists\n",
    "\n",
    "import pickle\n",
    "\n",
    "RESULTS_FOLDER = './intermediate_results/new_models_probs'\n",
    "\n",
    "results_path = join(RESULTS_FOLDER, 'bert_predictions.txt')\n",
    "\n",
    "# 3/27: https://stackoverflow.com/questions/27745500/how-to-save-a-list-to-a-file-and-read-it-as-a-list-type\n",
    "with open(results_path, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3193\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dev_csv_path = './new_models/dev_lm_sentence_input.csv'\n",
    "\n",
    "all_sentences = pd.read_csv(dev_csv_path)['user_candidate_transcription']\n",
    "print(len(all_sentences))\n",
    "num_select = 2\n",
    "sentences_subset = list(all_sentences.iloc[:num_select])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2\n",
      "Scoring with mode: single_word\n",
      "Index: 0\n"
     ]
    }
   ],
   "source": [
    "from new_models import gpt2_scores\n",
    "\n",
    "import importlib\n",
    "importlib.reload(gpt2_scores)\n",
    "\n",
    "#subset_scores = gpt2_scores.score_inputs(sentences_subset, mode = 'sentence', model_type = '')\n",
    "subset_scores_words = gpt2_scores.score_inputs(sentences_subset, mode = 'single_word', model_type = '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "from new_models import bert_prefix_scores\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "# 3/11: importlib help: https://stackoverflow.com/questions/1254370/reimport-a-module-in-python-while-interactive\n",
    "\n",
    "import importlib\n",
    "importlib.reload(bert_prefix_scores)\n",
    "\n",
    "bert_subset_scores = bert_prefix_scores.score_inputs(sentences_subset, mode = 'sentence')\n",
    "#bert_subset_scores = bert_prefix_scores.score_inputs(sentences_subset, mode = 'single_word')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_prefix_scores.score_inputs([], mode = 'sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# General sanity checks for trends -- the trends don't completely match up with expectation,\n",
    "#   but the ones with obvious differences (okay sentence structure vs. bad structure) do.\n",
    "sentence_cases = [\n",
    "    'apple toast bring not unsure test coding', # Should have high surprisal\n",
    "    'the person walked down the street', # Low surprisal\n",
    "    'I did not want to go to the library dolphin', # Should have medium surprisal due to last word.\n",
    "    'I would have preferred to eat libraries', # Should have medium surprisal due to last word.\n",
    "    'I coding test passed consequently did yes', # Should have high surprisal\n",
    "    'this sentence should have a low score', # Low surprisal (or medium after observing the results, because \"score\" is ML-specific)\n",
    "    'this sentence should have a paragraph', # Even lower surprisal -- observationally this isn't the case! Which is interesting.\n",
    "]\n",
    "\n",
    "# These results aren't really intuitive.\n",
    "\n",
    "results = bert_prefix_scores.score_inputs(sentence_cases, mode = 'sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sentence apple toast bring not unsure test coding\n",
      "\tThe probability score was 0.00039608879680080075\n",
      "For sentence the person walked down the street\n",
      "\tThe probability score was 0.2622058192888896\n",
      "For sentence I did not want to go to the library dolphin\n",
      "\tThe probability score was 0.6278652667999267\n",
      "For sentence I would have preferred to eat libraries\n",
      "\tThe probability score was 0.5334082671574184\n",
      "For sentence I coding test passed consequently did yes\n",
      "\tThe probability score was 0.0004042371043137142\n",
      "For sentence this sentence should have a low score\n",
      "\tThe probability score was 0.20211592742374965\n",
      "For sentence this sentence should have a paragraph\n",
      "\tThe probability score was 0.07603498796621959\n"
     ]
    }
   ],
   "source": [
    "for s, score in zip(sentence_cases, results):\n",
    "    print(f'For sentence {s}') # Note that above analysis is surprisal -- this is probability.\n",
    "    print(f'\\tThe probability score was {score / len(s.split())}')\n",
    "    # These are not very intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'new_models.bertlike_funcs' from '/home/nwong/chompsky/serial_chain/telephone-analysis-public/new_models/bertlike_funcs.py'>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from new_models import bart_scores, bertlike_funcs\n",
    "import importlib\n",
    "\n",
    "importlib.reload(bart_scores)\n",
    "importlib.reload(bertlike_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires manual check here\n",
      "['<s>', '<mask>', 'Ġis', 'Ġgreat', 'Ġ.', '</s>']\n",
      "<mask>\n",
      "Requires manual check here\n",
      "['<s>', 'This', '<mask>', 'Ġgreat', 'Ġ.', '</s>']\n",
      "<mask>\n",
      "Requires manual check here\n",
      "['<s>', 'This', 'Ġis', '<mask>', 'Ġ.', '</s>']\n",
      "<mask>\n",
      "Requires manual check here\n",
      "['<s>', 'This', 'Ġis', 'Ġgreat', 'Ġ.', '<mask>']\n",
      "<mask>\n",
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "def test_get_positions_from_encoded():\n",
    "    \n",
    "    #3/28: https://huggingface.co/transformers/model_doc/bart.html\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "    sent = new_model_funcs.prepSentence(\"This is great\")\n",
    "    \n",
    "    expected = [\n",
    "            ['<s>', '<mask>', 'Ġis', 'Ġgreat', 'Ġ.', '</s>'],\n",
    "            ['<s>', 'This', '<mask>', 'Ġgreat', 'Ġ.', '</s>'],\n",
    "            ['<s>', 'This', 'Ġis', '<mask>', 'Ġ.', '</s>'],\n",
    "            ['<s>', 'This', 'Ġis', 'Ġgreat', 'Ġ.', '<mask>'], \n",
    "        ]\n",
    "    \n",
    "    num_tokens = len(expected[0])\n",
    "    \n",
    "    expected_pos = torch.Tensor(list(range(1, num_tokens - 2)) + [num_tokens -1]).long()\n",
    "    enc_t = torch.Tensor(tokenizer(sent)['input_ids']).unsqueeze(0)\n",
    "    \n",
    "    res_tokens, res_next_words, extract_positions = bart_scores.get_positions_from_encoded(enc_t, 50264)\n",
    "        \n",
    "    actual = [tokenizer.convert_ids_to_tokens(t) for t in res_tokens]\n",
    "    \n",
    "    for i, e in enumerate(extract_positions):\n",
    "        print('Requires manual check here')\n",
    "        e = int(e.item())\n",
    "        print(actual[i])\n",
    "        print(actual[i][e])\n",
    "\n",
    "    assert actual == expected\n",
    "    assert torch.all(extract_positions == expected_pos)\n",
    "    \n",
    "    print('Test passed')\n",
    "    \n",
    "test_get_positions_from_encoded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting most likely tokens to complete '['It']' in descending order\n",
      "         Word  Score value\n",
      "0          It      0.33303\n",
      "1         Now      0.22798\n",
      "2       Maybe      0.02875\n",
      "3          So      0.01979\n",
      "4           I      0.01654\n",
      "5        When      0.01612\n",
      "6     Finally      0.01580\n",
      "7       After      0.01427\n",
      "8         And      0.01296\n",
      "9        Well      0.00814\n",
      "10       Okay      0.00670\n",
      "11      Today      0.00656\n",
      "12         OK      0.00653\n",
      "13    Perhaps      0.00600\n",
      "14         If      0.00593\n",
      "15        But      0.00507\n",
      "16  Sometimes      0.00463\n",
      "17        now      0.00458\n",
      "18         it      0.00456\n",
      "19         We      0.00434\n",
      "Reporting most likely tokens to complete '['It', \"'s\"]' in descending order\n",
      "        Word  Score value\n",
      "0         âĢ      0.52275\n",
      "1         's      0.31575\n",
      "2        Ġis      0.05309\n",
      "3       Ġwas      0.04933\n",
      "4     Ġmight      0.00838\n",
      "5     Ġseems      0.00621\n",
      "6       Ġmay      0.00430\n",
      "7    Ġreally      0.00272\n",
      "8     Ġnever      0.00257\n",
      "9   Ġfinally      0.00196\n",
      "10     Ġjust      0.00190\n",
      "11      Ġhas      0.00143\n",
      "12    Ġdoesn      0.00142\n",
      "13    Ġtakes      0.00138\n",
      "14      Ġisn      0.00111\n",
      "15    Ġwould      0.00101\n",
      "16   Ġalmost      0.00099\n",
      "17   Ġshould      0.00087\n",
      "18     Ġwill      0.00086\n",
      "19     Ġwasn      0.00081\n",
      "Reporting most likely tokens to complete '['It', \"'s\", 'Ġtime']' in descending order\n",
      "       Word  Score value\n",
      "0     Ġtime      0.10469\n",
      "1        Ġa      0.09893\n",
      "2      Ġnot      0.09342\n",
      "3   Ġalways      0.04027\n",
      "4     Ġeasy      0.02489\n",
      "5       Ġso      0.02317\n",
      "6   Ġalmost      0.02289\n",
      "7    Ġnever      0.02171\n",
      "8     Ġjust      0.02168\n",
      "9     Ġhard      0.01954\n",
      "10     Ġthe      0.01880\n",
      "11  Ġreally      0.01542\n",
      "12     Ġtoo      0.01261\n",
      "13   Ġworth      0.01076\n",
      "14   Ġstill      0.01068\n",
      "15  Ġpretty      0.01008\n",
      "16     Ġall      0.00968\n",
      "17      Ġan      0.00966\n",
      "18    Ġvery      0.00909\n",
      "19    Ġalso      0.00876\n",
      "Reporting most likely tokens to complete '['It', \"'s\", 'Ġtime', 'Ġto']' in descending order\n",
      "        Word  Score value\n",
      "0        Ġto      0.90629\n",
      "1       Ġfor      0.07281\n",
      "2        Ġwe      0.00402\n",
      "3       Ġyou      0.00396\n",
      "4         ĠI      0.00196\n",
      "5          ,      0.00132\n",
      "6       Ġnow      0.00076\n",
      "7       Ġnot      0.00052\n",
      "8      Ġthat      0.00044\n",
      "9          .      0.00043\n",
      "10      Ġthe      0.00042\n",
      "11    Ġagain      0.00040\n",
      "12     Ġthey      0.00029\n",
      "13  Ġsomeone      0.00021\n",
      "14         :      0.00018\n",
      "15         !      0.00018\n",
      "16     Ġthis      0.00016\n",
      "17        Ġ.      0.00015\n",
      "18       ĠTo      0.00012\n",
      "19        Ġi      0.00011\n",
      "Reporting most likely tokens to complete '['It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo']' in descending order\n",
      "       Word  Score value\n",
      "0       Ġgo      0.21521\n",
      "1      Ġget      0.18219\n",
      "2   Ġreturn      0.10987\n",
      "3     Ġhead      0.04640\n",
      "4     Ġtake      0.03615\n",
      "5     Ġmake      0.03084\n",
      "6     Ġmove      0.02994\n",
      "7    Ġbring      0.02171\n",
      "8     Ġcome      0.02154\n",
      "9    Ġstart      0.01523\n",
      "10     Ġsay      0.01511\n",
      "11     Ġadd      0.01261\n",
      "12    Ġcall      0.00865\n",
      "13    Ġgive      0.00794\n",
      "14    Ġturn      0.00779\n",
      "15    Ġstop      0.00669\n",
      "16    Ġsend      0.00530\n",
      "17     Ġput      0.00527\n",
      "18  Ġswitch      0.00516\n",
      "19     Ġpay      0.00473\n",
      "Reporting most likely tokens to complete '['It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo', 'Ġto']' in descending order\n",
      "         Word  Score value\n",
      "0         Ġto      0.29737\n",
      "1       Ġback      0.26746\n",
      "2       Ġhome      0.06561\n",
      "3        Ġout      0.04329\n",
      "4     Ġinside      0.02922\n",
      "5           .      0.02066\n",
      "6       Ġinto      0.01559\n",
      "7        Ġand      0.01375\n",
      "8        Ġget      0.01229\n",
      "9         Ġin      0.00988\n",
      "10     Ġcheck      0.00932\n",
      "11  Ġshopping      0.00885\n",
      "12       Ġthe      0.00750\n",
      "13       Ġbuy      0.00746\n",
      "14       Ġsee      0.00658\n",
      "15       Ġfor      0.00655\n",
      "16   Ġoutside      0.00612\n",
      "17          ,      0.00540\n",
      "18      Ġfind      0.00461\n",
      "19          !      0.00359\n",
      "Reporting most likely tokens to complete '['It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo', 'Ġto', 'Ġthe']' in descending order\n",
      "          Word  Score value\n",
      "0         Ġthe      0.75365\n",
      "1           Ġa      0.10249\n",
      "2        Ġyour      0.02405\n",
      "3     Ġanother      0.01003\n",
      "4          Ġan      0.00933\n",
      "5        Ġwork      0.00883\n",
      "6          Ġmy      0.00822\n",
      "7         Ġbed      0.00819\n",
      "8        Ġthat      0.00630\n",
      "9       Ġsleep      0.00334\n",
      "10        Ġour      0.00310\n",
      "11      Ġstore      0.00288\n",
      "12       Ġthis      0.00285\n",
      "13    ĠWalmart      0.00131\n",
      "14     ĠTarget      0.00127\n",
      "15  ĠStarbucks      0.00109\n",
      "16        ĠThe      0.00101\n",
      "17       Ġbook      0.00087\n",
      "18      ĠApple      0.00071\n",
      "19     Ġschool      0.00068\n",
      "Reporting most likely tokens to complete '['It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo', 'Ġto', 'Ġthe', 'Ġstore']' in descending order\n",
      "         Word  Score value\n",
      "0        Ġgym      0.07268\n",
      "1   Ġbathroom      0.03674\n",
      "2     Ġmovies      0.02923\n",
      "3      Ġbeach      0.02778\n",
      "4       Ġnext      0.02590\n",
      "5      Ġpolls      0.02584\n",
      "6       Ġdump      0.02390\n",
      "7    Ġlibrary      0.02143\n",
      "8     Ġtoilet      0.01519\n",
      "9      Ġgrave      0.01264\n",
      "10   Ġdentist      0.01197\n",
      "11    Ġoffice      0.01074\n",
      "12   Ġdrawing      0.01036\n",
      "13         Ġ.      0.00992\n",
      "14       Ġbig      0.00883\n",
      "15      Ġpark      0.00818\n",
      "16     Ġstore      0.00769\n",
      "17      Ġroot      0.00749\n",
      "18      Ġbank      0.00733\n",
      "19   Ġgrocery      0.00565\n",
      "Reporting most likely tokens to complete '['It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo', 'Ġto', 'Ġthe', 'Ġstore']' in descending order\n",
      "      Word  Score value\n",
      "0       Ġ.      0.61523\n",
      "1      ĠIt      0.16321\n",
      "2       It      0.06159\n",
      "3        .      0.01253\n",
      "4    ĠTime      0.00772\n",
      "5     ĠThe      0.00626\n",
      "6     ĠNow      0.00461\n",
      "7     ĠYou      0.00438\n",
      "8      ĠWe      0.00431\n",
      "9     ĠLet      0.00415\n",
      "10      ĠI      0.00406\n",
      "11    ĠAnd      0.00383\n",
      "12     Ġit      0.00258\n",
      "13     The      0.00235\n",
      "14      We      0.00225\n",
      "15     You      0.00220\n",
      "16   ĠThat      0.00216\n",
      "17  ĠThere      0.00200\n",
      "18     Now      0.00194\n",
      "19     And      0.00155\n"
     ]
    }
   ],
   "source": [
    "def bart_prefix_predictions_single(this_sentence):\n",
    "\n",
    "    next_word_probs, next_words_actual, next_word_softmax = bart_scores.get_bart_probabilities(this_sentence, tokenizer, model, verifying = True)\n",
    "\n",
    "    raw_tokens = tokenizer.tokenize(this_sentence)\n",
    "\n",
    "    words = this_sentence.split()\n",
    "    for idx in range(0, next_word_probs.shape[0]):\n",
    "        # Note that need to negate surprisals to treat them like probabilities, as here.\n",
    "        results, _ = bertlike_funcs.report_mask_words(next_word_softmax[idx], raw_tokens[:idx+1], tokenizer)\n",
    "        print(results)\n",
    "        \n",
    "bart_prefix_predictions_single(\"It's time to go to the store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "For word: Each\n",
      "Softmax probability: 0.06420725584030151\n",
      "DF probability: 0.06420725584030151\n",
      "\tDifference: 0.0\n",
      "For word:  non\n",
      "Softmax probability: 0.057423967868089676\n",
      "DF probability: 0.057423967868089676\n",
      "\tDifference: 0.0\n",
      "For word:  book\n",
      "Softmax probability: 0.35598254203796387\n",
      "DF probability: 0.35598254203796387\n",
      "\tDifference: 0.0\n",
      "For word: </s>\n",
      "Softmax probability: 5.314640930009773e-06\n",
      "DF probability: 5.314640930009773e-06\n",
      "\tDifference: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Checking new_model_funcs, convert to df function.\n",
    "\n",
    "subset_scores_words = bart_scores.score_inputs(sentences_subset)\n",
    "\n",
    "sentence_idx = 1\n",
    "ttensor, tnext_words = bart_scores.get_bart_probabilities(sentences_subset[sentence_idx], tokenizer, model)\n",
    "\n",
    "word_idxs = [0, 1, 3, tnext_words.shape[0] - 1]\n",
    "for widx in word_idxs:\n",
    "    word = tokenizer.decode([tnext_words[widx]])\n",
    "    softmax_prob = ttensor[widx]\n",
    "    df_prob = subset_scores_words[sentence_idx].iloc[widx]['prob']\n",
    "     \n",
    "    print(f'For word: {word}')\n",
    "    print(f'Softmax probability: {softmax_prob}')\n",
    "    print(f'DF probability: {df_prob}') # How to index into the spot with the word?\n",
    "    print(f'\\tDifference: {softmax_prob - df_prob}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## More informal checks of correctness in BERT\n",
    "import torch\n",
    "\n",
    "#2/20: https://huggingface.co/transformers/quickstart.html\n",
    "\n",
    "# Please note that, for consistency with the standard \"It's time to go to the\" check, I use bert-base-uncased here.\n",
    "# But the actual model used for the tests is the word tokenized one.\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('bert-large-uncased-whole-word-masking')\n",
    "model.eval()\n",
    "    \n",
    "#2/20: https://albertauyeung.github.io/2020/06/19/bert-tokenization.html\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- NEW MODE ----------------\n",
      "Requires manual check here\n",
      "['[CLS]', '[MASK]', 'is', 'great', '.', '[SEP]']\n",
      "[MASK]\n",
      "Requires manual check here\n",
      "['[CLS]', 'this', '[MASK]', 'great', '.', '[SEP]']\n",
      "[MASK]\n",
      "Requires manual check here\n",
      "['[CLS]', 'this', 'is', '[MASK]', '.', '[SEP]']\n",
      "[MASK]\n",
      "---------- NEW MODE ----------------\n",
      "Requires manual check here\n",
      "['[CLS]', '[MASK]', 'is', 'great', '.', '[SEP]']\n",
      "[MASK]\n",
      "Requires manual check here\n",
      "['[CLS]', 'this', '[MASK]', 'great', '.', '[SEP]']\n",
      "[MASK]\n",
      "Requires manual check here\n",
      "['[CLS]', 'this', 'is', '[MASK]', '.', '[SEP]']\n",
      "[MASK]\n",
      "Requires manual check here\n",
      "['[CLS]', 'this', 'is', 'great', '.', '[MASK]']\n",
      "[MASK]\n",
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def test_get_positions_from_encoded():\n",
    "    \n",
    "    #2/20: https://albertauyeung.github.io/2020/06/19/bert-tokenization.html\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n",
    "\n",
    "    sent = \"This is great\"\n",
    "    \n",
    "    expected = {\n",
    "        'sentence': [\n",
    "            ['[CLS]', '[MASK]', 'is', 'great', '.', '[SEP]'],\n",
    "            ['[CLS]', 'this', '[MASK]', 'great', '.', '[SEP]'],\n",
    "            ['[CLS]', 'this', 'is', '[MASK]', '.', '[SEP]']\n",
    "        ],\n",
    "        'single_word': [\n",
    "            ['[CLS]', '[MASK]', 'is', 'great', '.', '[SEP]'],\n",
    "            ['[CLS]', 'this', '[MASK]', 'great', '.', '[SEP]'],\n",
    "            ['[CLS]', 'this', 'is', '[MASK]', '.', '[SEP]'],\n",
    "            ['[CLS]', 'this', 'is', 'great', '.', '[MASK]'], \n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    num_tokens = len(expected['sentence'][0])\n",
    "    \n",
    "    expected_pos = {\n",
    "        'sentence': torch.Tensor(list(range(1, num_tokens - 2))).long(),\n",
    "        'single_word' : torch.Tensor(list(range(1, num_tokens - 2)) + [num_tokens -1]).long()\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for mode in ['sentence', 'single_word']:\n",
    "        print('---------- NEW MODE ----------------')\n",
    "        enc_s, seg_s = bert_prefix_scores.get_encoded_text(sent, tokenizer)\n",
    "        res_tokens, res_segs, res_next_words, extract_positions = bert_prefix_scores.get_positions_from_encoded(enc_s, seg_s, 103, mode)\n",
    "        \n",
    "        actual = [tokenizer.convert_ids_to_tokens(t) for t in res_tokens]\n",
    "        for i, e in enumerate(extract_positions):\n",
    "            print('Requires manual check here')\n",
    "            e = int(e.item())\n",
    "            print(actual[i])\n",
    "            print(actual[i][e])\n",
    "\n",
    "        assert actual == expected[mode]\n",
    "        assert torch.all(extract_positions == expected_pos[mode])\n",
    "    \n",
    "    print('Test passed')\n",
    "    \n",
    "test_get_positions_from_encoded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "For word: each\n",
      "Softmax probability: 0.4613463580608368\n",
      "DF probability: 0.4613463580608368\n",
      "\tDifference: 0.0\n",
      "For word: non\n",
      "Softmax probability: 0.007277762051671743\n",
      "DF probability: 0.007277762051671743\n",
      "\tDifference: 0.0\n",
      "For word: book\n",
      "Softmax probability: 0.5484470725059509\n",
      "DF probability: 0.5484470725059509\n",
      "\tDifference: 0.0\n",
      "For word: [SEP]\n",
      "Softmax probability: 7.019330610091856e-07\n",
      "DF probability: 7.019330610091856e-07\n",
      "\tDifference: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Checking new_model_funcs, convert to df function.\n",
    "from new_models import bert_scores\n",
    "\n",
    "subset_scores_words = bert_scores.score_inputs(sentences_subset, mode = 'single_word')\n",
    "\n",
    "sentence_idx = 1\n",
    "ttensor, tnext_words = bert_scores.get_bert_probabilities(sentences_subset[sentence_idx], tokenizer, model, 'single_word')\n",
    "\n",
    "word_idxs = [0, 1, 3, tnext_words.shape[0] - 1]\n",
    "for widx in word_idxs:\n",
    "    word = tokenizer.decode([tnext_words[widx]])\n",
    "    softmax_prob = ttensor[widx]\n",
    "    df_prob = subset_scores_words[sentence_idx].iloc[widx]['prob']\n",
    "     \n",
    "    print(f'For word: {word}')\n",
    "    print(f'Softmax probability: {softmax_prob}')\n",
    "    print(f'DF probability: {df_prob}') # How to index into the spot with the word?\n",
    "    print(f'\\tDifference: {softmax_prob - df_prob}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3/11: importlib help: https://stackoverflow.com/questions/1254370/reimport-a-module-in-python-while-interactive\n",
    "import importlib\n",
    "importlib.reload(bert_prefix_scores)\n",
    "\n",
    "from new_models import new_model_funcs\n",
    "\n",
    "def prefix_predictions_single(this_sentence):\n",
    "\n",
    "    score, this_probs = bert_prefix_scores.get_bert_sentence_score(this_sentence, tokenizer, model, verifying = True)\n",
    "\n",
    "    raw_tokens = tokenizer.tokenize(this_sentence)\n",
    "\n",
    "    words = this_sentence.split()\n",
    "    for idx in range(0, this_probs.shape[0]):\n",
    "        # Note that need to negate surprisals to treat them like probabilities, as here.\n",
    "        results, _ = bert_prefix_scores.report_mask_words(this_probs[idx], raw_tokens[:idx+1], tokenizer)\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting most likely tokens to complete '['apple']' in descending order\n",
      "       Word  Score value\n",
      "0       the      0.18794\n",
      "1         a      0.06843\n",
      "2         \"      0.02393\n",
      "3         “      0.02370\n",
      "4     first      0.01813\n",
      "5         i      0.01780\n",
      "6       and      0.01361\n",
      "7      good      0.01353\n",
      "8      your      0.01345\n",
      "9        my      0.01294\n",
      "10      you      0.01213\n",
      "11        '      0.01207\n",
      "12     this      0.01188\n",
      "13  morning      0.01070\n",
      "14      our      0.01058\n",
      "15     make      0.00959\n",
      "16       we      0.00929\n",
      "17       no      0.00821\n",
      "18     some      0.00721\n",
      "19     warm      0.00624\n",
      "Reporting most likely tokens to complete '['apple', 'toast']' in descending order\n",
      "     Word  Score value\n",
      "0      to      0.21832\n",
      "1       ,      0.14313\n",
      "2       i      0.12641\n",
      "3     you      0.11488\n",
      "4    will      0.05468\n",
      "5       -      0.02836\n",
      "6       .      0.02442\n",
      "7   would      0.02088\n",
      "8      we      0.01847\n",
      "9     can      0.01621\n",
      "10   they      0.01315\n",
      "11      :      0.01178\n",
      "12    and      0.00888\n",
      "13   must      0.00847\n",
      "14     is      0.00814\n",
      "15  trees      0.00679\n",
      "16    she      0.00638\n",
      "17  could      0.00489\n",
      "18    for      0.00465\n",
      "19      ?      0.00447\n",
      "Reporting most likely tokens to complete '['apple', 'toast', 'bring']' in descending order\n",
      "          Word  Score value\n",
      "0         cake      0.02185\n",
      "1           no      0.01650\n",
      "2           vs      0.01589\n",
      "3            c      0.01403\n",
      "4     sandwich      0.01246\n",
      "5         soup      0.01145\n",
      "6      cookies      0.01131\n",
      "7       cereal      0.00931\n",
      "8      pudding      0.00858\n",
      "9            n      0.00756\n",
      "10         ltd      0.00745\n",
      "11         etc      0.00688\n",
      "12    biscuits      0.00669\n",
      "13           2      0.00659\n",
      "14      butter      0.00644\n",
      "15     special      0.00643\n",
      "16      sticks      0.00632\n",
      "17        club      0.00594\n",
      "18      crunch      0.00566\n",
      "19  sandwiches      0.00561\n",
      "Reporting most likely tokens to complete '['apple']' in descending order\n",
      "      Word  Score value\n",
      "0      the      0.23198\n",
      "1        a      0.04642\n",
      "2        \"      0.03231\n",
      "3     your      0.02402\n",
      "4     make      0.02276\n",
      "5        “      0.01961\n",
      "6       my      0.01383\n",
      "7     this      0.01329\n",
      "8    sweet      0.00994\n",
      "9    apple      0.00979\n",
      "10      no      0.00948\n",
      "11     and      0.00782\n",
      "12     our      0.00781\n",
      "13    what      0.00768\n",
      "14    good      0.00713\n",
      "15     eat      0.00704\n",
      "16     his      0.00695\n",
      "17    that      0.00679\n",
      "18    when      0.00647\n",
      "19  little      0.00616\n",
      "Reporting most likely tokens to complete '['apple', 'pie']' in descending order\n",
      "     Word  Score value\n",
      "0      to      0.21832\n",
      "1       ,      0.14313\n",
      "2       i      0.12641\n",
      "3     you      0.11488\n",
      "4    will      0.05468\n",
      "5       -      0.02836\n",
      "6       .      0.02442\n",
      "7   would      0.02088\n",
      "8      we      0.01847\n",
      "9     can      0.01621\n",
      "10   they      0.01315\n",
      "11      :      0.01178\n",
      "12    and      0.00888\n",
      "13   must      0.00847\n",
      "14     is      0.00814\n",
      "15  trees      0.00679\n",
      "16    she      0.00638\n",
      "17  could      0.00489\n",
      "18    for      0.00465\n",
      "19      ?      0.00447\n",
      "Reporting most likely tokens to complete '['apple', 'pie', 'bring']' in descending order\n",
      "        Word  Score value\n",
      "0   festival      0.01487\n",
      "1       soup      0.01168\n",
      "2         vs      0.01067\n",
      "3        inc      0.00776\n",
      "4   magazine      0.00756\n",
      "5      sales      0.00740\n",
      "6      music      0.00700\n",
      "7         no      0.00657\n",
      "8        etc      0.00648\n",
      "9        pie      0.00614\n",
      "10     world      0.00596\n",
      "11       ltd      0.00590\n",
      "12    market      0.00589\n",
      "13      wars      0.00547\n",
      "14      club      0.00538\n",
      "15   company      0.00518\n",
      "16     party      0.00517\n",
      "17     sauce      0.00514\n",
      "18   reviews      0.00512\n",
      "19        co      0.00482\n",
      "Reporting most likely tokens to complete '['apple']' in descending order\n",
      "       Word  Score value\n",
      "0       the      0.18794\n",
      "1         a      0.06843\n",
      "2         \"      0.02393\n",
      "3         “      0.02370\n",
      "4     first      0.01813\n",
      "5         i      0.01780\n",
      "6       and      0.01361\n",
      "7      good      0.01353\n",
      "8      your      0.01345\n",
      "9        my      0.01294\n",
      "10      you      0.01213\n",
      "11        '      0.01207\n",
      "12     this      0.01188\n",
      "13  morning      0.01070\n",
      "14      our      0.01058\n",
      "15     make      0.00959\n",
      "16       we      0.00929\n",
      "17       no      0.00821\n",
      "18     some      0.00721\n",
      "19     warm      0.00624\n",
      "Reporting most likely tokens to complete '['apple', 'toast']' in descending order\n",
      "     Word  Score value\n",
      "0      to      0.21832\n",
      "1       ,      0.14313\n",
      "2       i      0.12641\n",
      "3     you      0.11488\n",
      "4    will      0.05468\n",
      "5       -      0.02836\n",
      "6       .      0.02442\n",
      "7   would      0.02088\n",
      "8      we      0.01847\n",
      "9     can      0.01621\n",
      "10   they      0.01315\n",
      "11      :      0.01178\n",
      "12    and      0.00888\n",
      "13   must      0.00847\n",
      "14     is      0.00814\n",
      "15  trees      0.00679\n",
      "16    she      0.00638\n",
      "17  could      0.00489\n",
      "18    for      0.00465\n",
      "19      ?      0.00447\n",
      "Reporting most likely tokens to complete '['apple', 'toast', 'bring']' in descending order\n",
      "          Word  Score value\n",
      "0         cake      0.02185\n",
      "1           no      0.01650\n",
      "2           vs      0.01589\n",
      "3            c      0.01403\n",
      "4     sandwich      0.01246\n",
      "5         soup      0.01145\n",
      "6      cookies      0.01131\n",
      "7       cereal      0.00931\n",
      "8      pudding      0.00858\n",
      "9            n      0.00756\n",
      "10         ltd      0.00745\n",
      "11         etc      0.00688\n",
      "12    biscuits      0.00669\n",
      "13           2      0.00659\n",
      "14      butter      0.00644\n",
      "15     special      0.00643\n",
      "16      sticks      0.00632\n",
      "17        club      0.00594\n",
      "18      crunch      0.00566\n",
      "19  sandwiches      0.00561\n"
     ]
    }
   ],
   "source": [
    "# Standard sanity check\n",
    "#prefix_predictions_single(\"It's time to go to the store\") \n",
    "\n",
    "# Below: Trying to ensure that strange behavior\n",
    "#     on always choosing the ground truth as the highest next prediction is resolved.\n",
    "\n",
    "# 3/20: This is now fixed, there is no strange behavior.\n",
    "\n",
    "prefix_predictions_single(\"apple toast bring\")\n",
    "prefix_predictions_single(\"apple pie bring\")\n",
    "prefix_predictions_single(\"apple toast bring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2\n",
      "Traceback (most recent call last):\n",
      "  File \"gpt2_tests.py\", line 1, in <module>\n",
      "    import gpt2_scores\n",
      "  File \"/home/nwong/chompsky/serial_chain/telephone-analysis-public/new_models/gpt2_scores.py\", line 154\n",
      "    model_name = f'gpt2{model_type}'\n",
      "                                   ^\n",
      "SyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "\n",
    "#os.chdir('./new_models')\n",
    "#!python3 gpt2_tests.py test_get_sentence_prefixes # These might have been broken by BERT development/later changes to the code.\n",
    "#os.chdir('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## This is an updated positional test.\n",
    "\n",
    "\n",
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 2/26: https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel\n",
    "# GPT2LMHeadModel returns unnormalized probabilities over the next word -- requires softmax.\n",
    "\n",
    "# or, gpt-2{medium, large, xl}\n",
    "# 2/26: options from here https://huggingface.co/transformers/pretrained_models.html\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def test_get_sentence_prefixes():\n",
    "    \n",
    "    sentence = \"how are you\"\n",
    "    \n",
    "    expected_prefixes_dict = {\n",
    "        \n",
    "        'sentence': [\n",
    "            ['<|endoftext|>'],\n",
    "            ['<|endoftext|>', 'How'],\n",
    "            ['<|endoftext|>', 'How', ' are'],\n",
    "        ],\n",
    "        'single_word': [\n",
    "            ['<|endoftext|>'],\n",
    "            ['<|endoftext|>', 'How'],\n",
    "            ['<|endoftext|>', 'How', ' are'],\n",
    "            ['<|endoftext|>', 'How', ' are', ' you', '.'],\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    expected_next_words_dict = {\n",
    "        'sentence' : ['How', ' are', ' you'],\n",
    "        'single_word' : ['How', ' are', ' you', '<|endoftext|>'],\n",
    "    }\n",
    "    \n",
    "    for mode in ['sentence', 'single_word']:\n",
    "        prefixes, next_words = gpt2_scores.get_sentence_prefixes(sentence, tokenizer, mode = mode)\n",
    "\n",
    "        translated_prefixes = []\n",
    "        for prefix in prefixes:\n",
    "            translated_prefixes.append(list(map(tokenizer.decode, prefix)))\n",
    "        translated_next_words = list(map(tokenizer.decode, next_words.unsqueeze(1)))\n",
    "\n",
    "        expected_prefixes = expected_prefixes_dict[mode]\n",
    "        expected_next_words = expected_next_words_dict[mode]\n",
    "        # Don't predict on/include in prefix the final word, because want to omit influence of added punctuation.\n",
    "\n",
    "        assert expected_prefixes == translated_prefixes, f'In mode: {mode}'\n",
    "        assert expected_next_words == translated_next_words, f'In mode: {mode}'\n",
    "    \n",
    "test_get_sentence_prefixes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking new_model_funcs, convert to df function.\n",
    "\n",
    "from new_models import gpt2_scores\n",
    "\n",
    "import importlib\n",
    "importlib.reload(gpt2_scores)\n",
    "\n",
    "subset_scores_words = gpt2_scores.score_inputs(sentences_subset, mode = 'single_word', model_type = '')\n",
    "\n",
    "sentence_idx = 1\n",
    "ttensor, tnext_words = gpt2_scores.get_gpt2_target_word_probs(sentences_subset[sentence_idx], tokenizer, model, 'single_word')\n",
    "\n",
    "word_idxs = [0, 1, 3, tnext_words.shape[0] - 1]\n",
    "for widx in word_idxs:\n",
    "    word = tokenizer.decode([tnext_words[widx]])\n",
    "    softmax_prob = ttensor[widx]\n",
    "    df_prob = subset_scores_words[sentence_idx].iloc[widx]['prob']\n",
    "     \n",
    "    print(f'For word: {word}')\n",
    "    print(f'Softmax probability: {softmax_prob}')\n",
    "    print(f'DF probability: {df_prob}') # How to index into the spot with the word?\n",
    "    print(f'\\tDifference: {softmax_prob - df_prob}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telephone-env-3",
   "language": "python",
   "name": "telephone-env-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
