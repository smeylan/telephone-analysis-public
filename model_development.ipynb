{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for model development scratchwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# 3/27: https://huggingface.co/transformers/model_doc/bart.html\n",
    "sentence = \"It's time to go to the <mask>\"\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "batch = tokenizer(sentence, return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<s>It's time to go to the<mask></s>\"]\n"
     ]
    }
   ],
   "source": [
    "#3/27: https://huggingface.co/transformers/model_doc/bart.html\n",
    "\n",
    "decoded_batch = tokenizer.batch_decode(batch['input_ids'])\n",
    "\n",
    "# 3/27: LM reference https://huggingface.co/transformers/model_doc/bart.html#barttokenizer\n",
    "this_lm = model.generate(batch['input_ids'])\n",
    "    \n",
    "print(decoded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(this_lm) # How to extract the softmax itself?\n",
    "result = model.forward(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 50265])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word          prob\n",
      "0         each  4.720370e-01\n",
      "1   nonfiction  1.034481e-04\n",
      "2         book  6.232377e-03\n",
      "3          has  8.093700e-01\n",
      "4            a  9.429873e-01\n",
      "5         call  2.517068e-03\n",
      "6       number  2.460797e-02\n",
      "7           on  9.372336e-01\n",
      "8          its  3.873985e-01\n",
      "9        spine  9.837382e-03\n",
      "10       [SEP]  7.783939e-07\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join, exists\n",
    "\n",
    "import pickle\n",
    "\n",
    "RESULTS_FOLDER = './intermediate_results/new_models_probs'\n",
    "\n",
    "results_path = join(RESULTS_FOLDER, 'bert_predictions.txt')\n",
    "\n",
    "# 3/27: https://stackoverflow.com/questions/27745500/how-to-save-a-list-to-a-file-and-read-it-as-a-list-type\n",
    "with open(results_path, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3193\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dev_csv_path = './new_models/dev_lm_sentence_input.csv'\n",
    "\n",
    "all_sentences = pd.read_csv(dev_csv_path)['user_candidate_transcription']\n",
    "print(len(all_sentences))\n",
    "num_select = 2\n",
    "sentences_subset = list(all_sentences.iloc[:num_select])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2\n",
      "Scoring with mode: single_word\n",
      "Index: 0\n"
     ]
    }
   ],
   "source": [
    "from new_models import gpt2_scores\n",
    "\n",
    "import importlib\n",
    "importlib.reload(gpt2_scores)\n",
    "\n",
    "#subset_scores = gpt2_scores.score_inputs(sentences_subset, mode = 'sentence', model_type = '')\n",
    "subset_scores_words = gpt2_scores.score_inputs(sentences_subset, mode = 'single_word', model_type = '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "from new_models import bert_prefix_scores\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "# 3/11: importlib help: https://stackoverflow.com/questions/1254370/reimport-a-module-in-python-while-interactive\n",
    "\n",
    "import importlib\n",
    "importlib.reload(bert_prefix_scores)\n",
    "\n",
    "bert_subset_scores = bert_prefix_scores.score_inputs(sentences_subset, mode = 'sentence')\n",
    "#bert_subset_scores = bert_prefix_scores.score_inputs(sentences_subset, mode = 'single_word')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_prefix_scores.score_inputs([], mode = 'sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# General sanity checks for trends -- the trends don't completely match up with expectation,\n",
    "#   but the ones with obvious differences (okay sentence structure vs. bad structure) do.\n",
    "sentence_cases = [\n",
    "    'apple toast bring not unsure test coding', # Should have high surprisal\n",
    "    'the person walked down the street', # Low surprisal\n",
    "    'I did not want to go to the library dolphin', # Should have medium surprisal due to last word.\n",
    "    'I would have preferred to eat libraries', # Should have medium surprisal due to last word.\n",
    "    'I coding test passed consequently did yes', # Should have high surprisal\n",
    "    'this sentence should have a low score', # Low surprisal (or medium after observing the results, because \"score\" is ML-specific)\n",
    "    'this sentence should have a paragraph', # Even lower surprisal -- observationally this isn't the case! Which is interesting.\n",
    "]\n",
    "\n",
    "# These results aren't really intuitive.\n",
    "\n",
    "results = bert_prefix_scores.score_inputs(sentence_cases, mode = 'sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sentence apple toast bring not unsure test coding\n",
      "\tThe probability score was 0.00039608879680080075\n",
      "For sentence the person walked down the street\n",
      "\tThe probability score was 0.2622058192888896\n",
      "For sentence I did not want to go to the library dolphin\n",
      "\tThe probability score was 0.6278652667999267\n",
      "For sentence I would have preferred to eat libraries\n",
      "\tThe probability score was 0.5334082671574184\n",
      "For sentence I coding test passed consequently did yes\n",
      "\tThe probability score was 0.0004042371043137142\n",
      "For sentence this sentence should have a low score\n",
      "\tThe probability score was 0.20211592742374965\n",
      "For sentence this sentence should have a paragraph\n",
      "\tThe probability score was 0.07603498796621959\n"
     ]
    }
   ],
   "source": [
    "for s, score in zip(sentence_cases, results):\n",
    "    print(f'For sentence {s}') # Note that above analysis is surprisal -- this is probability.\n",
    "    print(f'\\tThe probability score was {score / len(s.split())}')\n",
    "    # These are not very intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## More informal checks of correctness in BERT\n",
    "import torch\n",
    "\n",
    "#2/20: https://huggingface.co/transformers/quickstart.html\n",
    "\n",
    "# Please note that, for consistency with the standard \"It's time to go to the\" check, I use bert-base-uncased here.\n",
    "# But the actual model used for the tests is the word tokenized one.\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('bert-large-uncased-whole-word-masking')\n",
    "model.eval()\n",
    "    \n",
    "#2/20: https://albertauyeung.github.io/2020/06/19/bert-tokenization.html\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- NEW MODE ----------------\n",
      "Requires manual check here\n",
      "['[CLS]', '[MASK]', 'is', 'great', '.', '[SEP]']\n",
      "[MASK]\n",
      "Requires manual check here\n",
      "['[CLS]', 'this', '[MASK]', 'great', '.', '[SEP]']\n",
      "[MASK]\n",
      "Requires manual check here\n",
      "['[CLS]', 'this', 'is', '[MASK]', '.', '[SEP]']\n",
      "[MASK]\n",
      "---------- NEW MODE ----------------\n",
      "Requires manual check here\n",
      "['[CLS]', '[MASK]', 'is', 'great', '.', '[SEP]']\n",
      "[MASK]\n",
      "Requires manual check here\n",
      "['[CLS]', 'this', '[MASK]', 'great', '.', '[SEP]']\n",
      "[MASK]\n",
      "Requires manual check here\n",
      "['[CLS]', 'this', 'is', '[MASK]', '.', '[SEP]']\n",
      "[MASK]\n",
      "Requires manual check here\n",
      "['[CLS]', 'this', 'is', 'great', '.', '[MASK]']\n",
      "[MASK]\n",
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def test_get_positions_from_encoded():\n",
    "    \n",
    "    #2/20: https://albertauyeung.github.io/2020/06/19/bert-tokenization.html\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n",
    "\n",
    "    sent = \"This is great\"\n",
    "    \n",
    "    expected = {\n",
    "        'sentence': [\n",
    "            ['[CLS]', '[MASK]', 'is', 'great', '.', '[SEP]'],\n",
    "            ['[CLS]', 'this', '[MASK]', 'great', '.', '[SEP]'],\n",
    "            ['[CLS]', 'this', 'is', '[MASK]', '.', '[SEP]']\n",
    "        ],\n",
    "        'single_word': [\n",
    "            ['[CLS]', '[MASK]', 'is', 'great', '.', '[SEP]'],\n",
    "            ['[CLS]', 'this', '[MASK]', 'great', '.', '[SEP]'],\n",
    "            ['[CLS]', 'this', 'is', '[MASK]', '.', '[SEP]'],\n",
    "            ['[CLS]', 'this', 'is', 'great', '.', '[MASK]'], \n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    num_tokens = len(expected['sentence'][0])\n",
    "    \n",
    "    expected_pos = {\n",
    "        'sentence': torch.Tensor(list(range(1, num_tokens - 2))).long(),\n",
    "        'single_word' : torch.Tensor(list(range(1, num_tokens - 2)) + [num_tokens -1]).long()\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for mode in ['sentence', 'single_word']:\n",
    "        print('---------- NEW MODE ----------------')\n",
    "        enc_s, seg_s = bert_prefix_scores.get_encoded_text(sent, tokenizer)\n",
    "        res_tokens, res_segs, res_next_words, extract_positions = bert_prefix_scores.get_positions_from_encoded(enc_s, seg_s, 103, mode)\n",
    "        \n",
    "        actual = [tokenizer.convert_ids_to_tokens(t) for t in res_tokens]\n",
    "        for i, e in enumerate(extract_positions):\n",
    "            print('Requires manual check here')\n",
    "            e = int(e.item())\n",
    "            print(actual[i])\n",
    "            print(actual[i][e])\n",
    "\n",
    "        assert actual == expected[mode]\n",
    "        assert torch.all(extract_positions == expected_pos[mode])\n",
    "    \n",
    "    print('Test passed')\n",
    "    \n",
    "test_get_positions_from_encoded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "For word: each\n",
      "Softmax probability: 0.4613463580608368\n",
      "DF probability: 0.4613463580608368\n",
      "\tDifference: 0.0\n",
      "For word: non\n",
      "Softmax probability: 0.007277762051671743\n",
      "DF probability: 0.007277762051671743\n",
      "\tDifference: 0.0\n",
      "For word: book\n",
      "Softmax probability: 0.5484470725059509\n",
      "DF probability: 0.5484470725059509\n",
      "\tDifference: 0.0\n",
      "For word: [SEP]\n",
      "Softmax probability: 7.019330610091856e-07\n",
      "DF probability: 7.019330610091856e-07\n",
      "\tDifference: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Checking new_model_funcs, convert to df function.\n",
    "from new_models import bert_scores\n",
    "\n",
    "subset_scores_words = bert_scores.score_inputs(sentences_subset, mode = 'single_word')\n",
    "\n",
    "sentence_idx = 1\n",
    "ttensor, tnext_words = bert_scores.get_bert_probabilities(sentences_subset[sentence_idx], tokenizer, model, 'single_word')\n",
    "\n",
    "word_idxs = [0, 1, 3, tnext_words.shape[0] - 1]\n",
    "for widx in word_idxs:\n",
    "    word = tokenizer.decode([tnext_words[widx]])\n",
    "    softmax_prob = ttensor[widx]\n",
    "    df_prob = subset_scores_words[sentence_idx].iloc[widx]['prob']\n",
    "     \n",
    "    print(f'For word: {word}')\n",
    "    print(f'Softmax probability: {softmax_prob}')\n",
    "    print(f'DF probability: {df_prob}') # How to index into the spot with the word?\n",
    "    print(f'\\tDifference: {softmax_prob - df_prob}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3/11: importlib help: https://stackoverflow.com/questions/1254370/reimport-a-module-in-python-while-interactive\n",
    "import importlib\n",
    "importlib.reload(bert_prefix_scores)\n",
    "\n",
    "from new_models import new_model_funcs\n",
    "\n",
    "def prefix_predictions_single(this_sentence):\n",
    "\n",
    "    score, this_probs = bert_prefix_scores.get_bert_sentence_score(this_sentence, tokenizer, model, verifying = True)\n",
    "\n",
    "    raw_tokens = tokenizer.tokenize(this_sentence)\n",
    "\n",
    "    words = this_sentence.split()\n",
    "    for idx in range(0, this_probs.shape[0]):\n",
    "        # Note that need to negate surprisals to treat them like probabilities, as here.\n",
    "        results, _ = bert_prefix_scores.report_mask_words(this_probs[idx], raw_tokens[:idx+1], tokenizer)\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting most likely tokens to complete '['apple']' in descending order\n",
      "       Word  Score value\n",
      "0       the      0.18794\n",
      "1         a      0.06843\n",
      "2         \"      0.02393\n",
      "3         “      0.02370\n",
      "4     first      0.01813\n",
      "5         i      0.01780\n",
      "6       and      0.01361\n",
      "7      good      0.01353\n",
      "8      your      0.01345\n",
      "9        my      0.01294\n",
      "10      you      0.01213\n",
      "11        '      0.01207\n",
      "12     this      0.01188\n",
      "13  morning      0.01070\n",
      "14      our      0.01058\n",
      "15     make      0.00959\n",
      "16       we      0.00929\n",
      "17       no      0.00821\n",
      "18     some      0.00721\n",
      "19     warm      0.00624\n",
      "Reporting most likely tokens to complete '['apple', 'toast']' in descending order\n",
      "     Word  Score value\n",
      "0      to      0.21832\n",
      "1       ,      0.14313\n",
      "2       i      0.12641\n",
      "3     you      0.11488\n",
      "4    will      0.05468\n",
      "5       -      0.02836\n",
      "6       .      0.02442\n",
      "7   would      0.02088\n",
      "8      we      0.01847\n",
      "9     can      0.01621\n",
      "10   they      0.01315\n",
      "11      :      0.01178\n",
      "12    and      0.00888\n",
      "13   must      0.00847\n",
      "14     is      0.00814\n",
      "15  trees      0.00679\n",
      "16    she      0.00638\n",
      "17  could      0.00489\n",
      "18    for      0.00465\n",
      "19      ?      0.00447\n",
      "Reporting most likely tokens to complete '['apple', 'toast', 'bring']' in descending order\n",
      "          Word  Score value\n",
      "0         cake      0.02185\n",
      "1           no      0.01650\n",
      "2           vs      0.01589\n",
      "3            c      0.01403\n",
      "4     sandwich      0.01246\n",
      "5         soup      0.01145\n",
      "6      cookies      0.01131\n",
      "7       cereal      0.00931\n",
      "8      pudding      0.00858\n",
      "9            n      0.00756\n",
      "10         ltd      0.00745\n",
      "11         etc      0.00688\n",
      "12    biscuits      0.00669\n",
      "13           2      0.00659\n",
      "14      butter      0.00644\n",
      "15     special      0.00643\n",
      "16      sticks      0.00632\n",
      "17        club      0.00594\n",
      "18      crunch      0.00566\n",
      "19  sandwiches      0.00561\n",
      "Reporting most likely tokens to complete '['apple']' in descending order\n",
      "      Word  Score value\n",
      "0      the      0.23198\n",
      "1        a      0.04642\n",
      "2        \"      0.03231\n",
      "3     your      0.02402\n",
      "4     make      0.02276\n",
      "5        “      0.01961\n",
      "6       my      0.01383\n",
      "7     this      0.01329\n",
      "8    sweet      0.00994\n",
      "9    apple      0.00979\n",
      "10      no      0.00948\n",
      "11     and      0.00782\n",
      "12     our      0.00781\n",
      "13    what      0.00768\n",
      "14    good      0.00713\n",
      "15     eat      0.00704\n",
      "16     his      0.00695\n",
      "17    that      0.00679\n",
      "18    when      0.00647\n",
      "19  little      0.00616\n",
      "Reporting most likely tokens to complete '['apple', 'pie']' in descending order\n",
      "     Word  Score value\n",
      "0      to      0.21832\n",
      "1       ,      0.14313\n",
      "2       i      0.12641\n",
      "3     you      0.11488\n",
      "4    will      0.05468\n",
      "5       -      0.02836\n",
      "6       .      0.02442\n",
      "7   would      0.02088\n",
      "8      we      0.01847\n",
      "9     can      0.01621\n",
      "10   they      0.01315\n",
      "11      :      0.01178\n",
      "12    and      0.00888\n",
      "13   must      0.00847\n",
      "14     is      0.00814\n",
      "15  trees      0.00679\n",
      "16    she      0.00638\n",
      "17  could      0.00489\n",
      "18    for      0.00465\n",
      "19      ?      0.00447\n",
      "Reporting most likely tokens to complete '['apple', 'pie', 'bring']' in descending order\n",
      "        Word  Score value\n",
      "0   festival      0.01487\n",
      "1       soup      0.01168\n",
      "2         vs      0.01067\n",
      "3        inc      0.00776\n",
      "4   magazine      0.00756\n",
      "5      sales      0.00740\n",
      "6      music      0.00700\n",
      "7         no      0.00657\n",
      "8        etc      0.00648\n",
      "9        pie      0.00614\n",
      "10     world      0.00596\n",
      "11       ltd      0.00590\n",
      "12    market      0.00589\n",
      "13      wars      0.00547\n",
      "14      club      0.00538\n",
      "15   company      0.00518\n",
      "16     party      0.00517\n",
      "17     sauce      0.00514\n",
      "18   reviews      0.00512\n",
      "19        co      0.00482\n",
      "Reporting most likely tokens to complete '['apple']' in descending order\n",
      "       Word  Score value\n",
      "0       the      0.18794\n",
      "1         a      0.06843\n",
      "2         \"      0.02393\n",
      "3         “      0.02370\n",
      "4     first      0.01813\n",
      "5         i      0.01780\n",
      "6       and      0.01361\n",
      "7      good      0.01353\n",
      "8      your      0.01345\n",
      "9        my      0.01294\n",
      "10      you      0.01213\n",
      "11        '      0.01207\n",
      "12     this      0.01188\n",
      "13  morning      0.01070\n",
      "14      our      0.01058\n",
      "15     make      0.00959\n",
      "16       we      0.00929\n",
      "17       no      0.00821\n",
      "18     some      0.00721\n",
      "19     warm      0.00624\n",
      "Reporting most likely tokens to complete '['apple', 'toast']' in descending order\n",
      "     Word  Score value\n",
      "0      to      0.21832\n",
      "1       ,      0.14313\n",
      "2       i      0.12641\n",
      "3     you      0.11488\n",
      "4    will      0.05468\n",
      "5       -      0.02836\n",
      "6       .      0.02442\n",
      "7   would      0.02088\n",
      "8      we      0.01847\n",
      "9     can      0.01621\n",
      "10   they      0.01315\n",
      "11      :      0.01178\n",
      "12    and      0.00888\n",
      "13   must      0.00847\n",
      "14     is      0.00814\n",
      "15  trees      0.00679\n",
      "16    she      0.00638\n",
      "17  could      0.00489\n",
      "18    for      0.00465\n",
      "19      ?      0.00447\n",
      "Reporting most likely tokens to complete '['apple', 'toast', 'bring']' in descending order\n",
      "          Word  Score value\n",
      "0         cake      0.02185\n",
      "1           no      0.01650\n",
      "2           vs      0.01589\n",
      "3            c      0.01403\n",
      "4     sandwich      0.01246\n",
      "5         soup      0.01145\n",
      "6      cookies      0.01131\n",
      "7       cereal      0.00931\n",
      "8      pudding      0.00858\n",
      "9            n      0.00756\n",
      "10         ltd      0.00745\n",
      "11         etc      0.00688\n",
      "12    biscuits      0.00669\n",
      "13           2      0.00659\n",
      "14      butter      0.00644\n",
      "15     special      0.00643\n",
      "16      sticks      0.00632\n",
      "17        club      0.00594\n",
      "18      crunch      0.00566\n",
      "19  sandwiches      0.00561\n"
     ]
    }
   ],
   "source": [
    "# Standard sanity check\n",
    "#prefix_predictions_single(\"It's time to go to the store\") \n",
    "\n",
    "# Below: Trying to ensure that strange behavior\n",
    "#     on always choosing the ground truth as the highest next prediction is resolved.\n",
    "\n",
    "# 3/20: This is now fixed, there is no strange behavior.\n",
    "\n",
    "prefix_predictions_single(\"apple toast bring\")\n",
    "prefix_predictions_single(\"apple pie bring\")\n",
    "prefix_predictions_single(\"apple toast bring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2\n",
      "Traceback (most recent call last):\n",
      "  File \"gpt2_tests.py\", line 1, in <module>\n",
      "    import gpt2_scores\n",
      "  File \"/home/nwong/chompsky/serial_chain/telephone-analysis-public/new_models/gpt2_scores.py\", line 154\n",
      "    model_name = f'gpt2{model_type}'\n",
      "                                   ^\n",
      "SyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "\n",
    "#os.chdir('./new_models')\n",
    "#!python3 gpt2_tests.py test_get_sentence_prefixes # These might have been broken by BERT development/later changes to the code.\n",
    "#os.chdir('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## This is an updated positional test.\n",
    "\n",
    "\n",
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 2/26: https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel\n",
    "# GPT2LMHeadModel returns unnormalized probabilities over the next word -- requires softmax.\n",
    "\n",
    "# or, gpt-2{medium, large, xl}\n",
    "# 2/26: options from here https://huggingface.co/transformers/pretrained_models.html\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def test_get_sentence_prefixes():\n",
    "    \n",
    "    sentence = \"how are you\"\n",
    "    \n",
    "    expected_prefixes_dict = {\n",
    "        \n",
    "        'sentence': [\n",
    "            ['<|endoftext|>'],\n",
    "            ['<|endoftext|>', 'How'],\n",
    "            ['<|endoftext|>', 'How', ' are'],\n",
    "        ],\n",
    "        'single_word': [\n",
    "            ['<|endoftext|>'],\n",
    "            ['<|endoftext|>', 'How'],\n",
    "            ['<|endoftext|>', 'How', ' are'],\n",
    "            ['<|endoftext|>', 'How', ' are', ' you', '.'],\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    expected_next_words_dict = {\n",
    "        'sentence' : ['How', ' are', ' you'],\n",
    "        'single_word' : ['How', ' are', ' you', '<|endoftext|>'],\n",
    "    }\n",
    "    \n",
    "    for mode in ['sentence', 'single_word']:\n",
    "        prefixes, next_words = gpt2_scores.get_sentence_prefixes(sentence, tokenizer, mode = mode)\n",
    "\n",
    "        translated_prefixes = []\n",
    "        for prefix in prefixes:\n",
    "            translated_prefixes.append(list(map(tokenizer.decode, prefix)))\n",
    "        translated_next_words = list(map(tokenizer.decode, next_words.unsqueeze(1)))\n",
    "\n",
    "        expected_prefixes = expected_prefixes_dict[mode]\n",
    "        expected_next_words = expected_next_words_dict[mode]\n",
    "        # Don't predict on/include in prefix the final word, because want to omit influence of added punctuation.\n",
    "\n",
    "        assert expected_prefixes == translated_prefixes, f'In mode: {mode}'\n",
    "        assert expected_next_words == translated_next_words, f'In mode: {mode}'\n",
    "    \n",
    "test_get_sentence_prefixes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking new_model_funcs, convert to df function.\n",
    "\n",
    "from new_models import gpt2_scores\n",
    "\n",
    "import importlib\n",
    "importlib.reload(gpt2_scores)\n",
    "\n",
    "subset_scores_words = gpt2_scores.score_inputs(sentences_subset, mode = 'single_word', model_type = '')\n",
    "\n",
    "sentence_idx = 1\n",
    "ttensor, tnext_words = gpt2_scores.get_gpt2_target_word_probs(sentences_subset[sentence_idx], tokenizer, model, 'single_word')\n",
    "\n",
    "word_idxs = [0, 1, 3, tnext_words.shape[0] - 1]\n",
    "for widx in word_idxs:\n",
    "    word = tokenizer.decode([tnext_words[widx]])\n",
    "    softmax_prob = ttensor[widx]\n",
    "    df_prob = subset_scores_words[sentence_idx].iloc[widx]['prob']\n",
    "     \n",
    "    print(f'For word: {word}')\n",
    "    print(f'Softmax probability: {softmax_prob}')\n",
    "    print(f'DF probability: {df_prob}') # How to index into the spot with the word?\n",
    "    print(f'\\tDifference: {softmax_prob - df_prob}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telephone-env-3",
   "language": "python",
   "name": "telephone-env-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
