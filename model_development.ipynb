{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other scratchwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model: gpt2_normal, sentence_scores: [-26.00150438142618, -30.140151837593834]\n",
      "For model: gpt2_medium, sentence_scores: [-23.961808500186287, -28.670494422365497]\n",
      "For model: bert, sentence_scores: [-13.294641712998299, -11.777179995095807]\n",
      "For model: bart, sentence_scores: [-16.233867877729487, -21.894162977643067]\n"
     ]
    }
   ],
   "source": [
    "# Looking at test saves for the model scores\n",
    "\n",
    "import os\n",
    "from os.path import join, exists\n",
    "\n",
    "from new_models import prep_probs\n",
    "\n",
    "RESULTS_FOLDER = './intermediate_results/new_models_probs'\n",
    "\n",
    "temp_results = {}\n",
    "for model_name in ['gpt2_normal', 'gpt2_medium', 'bert', 'bart']:\n",
    "    temp_results[model_name] = prep_probs.load_word_scores(model_name, RESULTS_FOLDER)\n",
    "    sentence_score = prep_probs.load_sentence_scores(model_name, RESULTS_FOLDER)\n",
    "    print(f'For model: {model_name}, sentence_scores: {sentence_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checks and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 0\n",
      "It 0.00911561120301485\n",
      "'s 0.4054446220397949\n",
      "Ġtime 0.034498848021030426\n",
      "Ġto 0.548720121383667\n",
      "Ġgo 0.015521024353802204\n",
      "Ġto 0.08927261829376221\n",
      "Ġthe 0.1976589858531952\n",
      "Ġstore 0.019540343433618546\n",
      "Ġ. 0.0002333719312446192\n",
      "<|endoftext|> 0.004117126576602459\n",
      "Walk 3.4847416827687994e-05\n",
      "Ġslowly 0.00015021645231172442\n",
      "Ġto 0.03525398299098015\n",
      "Ġthe 0.47555652260780334\n",
      "Ġgolden 4.06051694881171e-05\n",
      "Ġstair 0.006223683711141348\n",
      "Ġ. 3.163226574542932e-05\n",
      "<|endoftext|> 0.0008614695398136973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[            word      prob\n",
       " 0             It  0.009116\n",
       " 1             's  0.405445\n",
       " 2          Ġtime  0.034499\n",
       " 3            Ġto  0.548720\n",
       " 4            Ġgo  0.015521\n",
       " 5            Ġto  0.089273\n",
       " 6           Ġthe  0.197659\n",
       " 7         Ġstore  0.019540\n",
       " 8             Ġ.  0.000233\n",
       " 9  <|endoftext|>  0.004117,             word      prob\n",
       " 0           Walk  0.000035\n",
       " 1        Ġslowly  0.000150\n",
       " 2            Ġto  0.035254\n",
       " 3           Ġthe  0.475557\n",
       " 4        Ġgolden  0.000041\n",
       " 5         Ġstair  0.006224\n",
       " 6             Ġ.  0.000032\n",
       " 7  <|endoftext|>  0.000861]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import new_models\n",
    "from new_models import model_score_funcs\n",
    "\n",
    "\n",
    "import importlib\n",
    "importlib.reload(model_score_funcs)\n",
    "\n",
    "\n",
    "inputs = [\n",
    "    \"it's time to go to the store\",\n",
    "    \"walk slowly to the golden stair\"\n",
    "]\n",
    "\n",
    "model_score_funcs.get_gpt2_scores(inputs)\n",
    "#model_score_funcs.get_bert_scores(inputs)\n",
    "#model_score_funcs.get_bart_scores(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertForMaskedLM, BertTokenizer, BartForConditionalGeneration, BartTokenizer\n",
    "from new_models import model_prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "models = {\n",
    "    'gpt2': GPT2LMHeadModel.from_pretrained('gpt2'),\n",
    "    'bert': BertForMaskedLM.from_pretrained('bert-large-uncased-whole-word-masking'),\n",
    "    'bart': BartForConditionalGeneration.from_pretrained('facebook/bart-base'),\n",
    "    \n",
    "}\n",
    "\n",
    "tokenizers = {\n",
    "    'gpt2': GPT2Tokenizer.from_pretrained('gpt2'),\n",
    "    'bert': BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking\"),\n",
    "    'bart': BartTokenizer.from_pretrained(\"facebook/bart-base\"),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting most likely tokens to complete 'it's time to go to the [MASK]' in descending order\n",
      "Ground truth probability, manual extract, for the word moon: 0.008538895286619663\n",
      "        Word  Score value\n",
      "0   hospital      0.06366\n",
      "1      beach      0.03984\n",
      "2   cemetery      0.03216\n",
      "3   bathroom      0.02819\n",
      "4      party      0.02643\n",
      "5     movies      0.02543\n",
      "6    meeting      0.02479\n",
      "7    airport      0.01794\n",
      "8     office      0.01390\n",
      "9    library      0.01348\n",
      "10       zoo      0.01343\n",
      "11    church      0.01283\n",
      "12    doctor      0.01203\n",
      "13    rescue      0.01107\n",
      "14   funeral      0.01032\n",
      "15      city      0.01001\n",
      "16      park      0.00918\n",
      "17   station      0.00917\n",
      "18      moon      0.00854\n",
      "19    museum      0.00797\n"
     ]
    }
   ],
   "source": [
    "## Testing the next model probabilities\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from new_models import model_score_utils\n",
    "\n",
    "importlib.reload(model_score_utils)\n",
    "\n",
    "\n",
    "def report_mask_words(scores, sentence, tokenizer):\n",
    "    \"\"\"\n",
    "    raw_scores = a (vocabulary,) tensor of selected softmax values for a pre-selected position.\n",
    "    mask_idx, the position to select for analysis.\n",
    "    \n",
    "    sentence = the prefix to do the prediction on\n",
    "    tokenizer = BERT/BART tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # It should intake the raw scores itself.\n",
    "    score_vals, word_idxs = torch.sort(scores, descending = True)\n",
    "    words = tokenizer.convert_ids_to_tokens(word_idxs)\n",
    "\n",
    "    print(f\"Reporting most likely tokens to complete '{sentence}' in descending order\")\n",
    "\n",
    "    num_report = 20\n",
    "\n",
    "    score_df = pd.DataFrame.from_dict({\n",
    "      'Word': words,\n",
    "      'Score value': list(map(lambda x : round(x, 5), score_vals.numpy().tolist()))\n",
    "      })\n",
    "\n",
    "    return score_df[:num_report]\n",
    "\n",
    "\n",
    "def test_gpt2_model_probs():\n",
    "    \n",
    "    \n",
    "    print('Note that this outputs softmax of the last word before the punctuation.')\n",
    "    \n",
    "    model = models['gpt2']; tok = tokenizers['gpt2']\n",
    "    test_sentence = \"it's time to go to the\"\n",
    "    # Below line is from the prefix code\n",
    "    test_sentence = f'{tok.bos_token}{test_sentence}{tok.eos_token}'\n",
    "    \n",
    "    this_tokens = tok.encode(test_sentence)\n",
    "    \n",
    "    # The 0 is a filler index.\n",
    "    this_pred_pos = len(this_tokens) - 2\n",
    "    _, probs = model_score_utils.get_model_probabilities(this_tokens, model, 0, this_pred_pos, verifying = True)\n",
    "    result_df = report_mask_words(probs, test_sentence, tok)\n",
    "    \n",
    "    test_word = 'Ġbathroom'\n",
    "    ground_truth_token = tok.convert_tokens_to_ids(test_word)\n",
    "    \n",
    "    print(f'Ground truth probability, manual extract, for the word {test_word}: {probs[ground_truth_token]}')\n",
    "    \n",
    "    return result_df\n",
    "    \n",
    "    \n",
    "def test_bertlike_model_probs(model_name):\n",
    "        \n",
    "    model = models[model_name]; tok = tokenizers[model_name]\n",
    "    \n",
    "    test_sentence = f\"it's time to go to the {tok.mask_token}\"\n",
    "    this_tokens = tok.encode(test_sentence)\n",
    "    this_pred_pos = len(this_tokens) - 2\n",
    "    \n",
    "    # The 0 is a filler index.\n",
    "    prob_at_ground_truth, probs = model_score_utils.get_model_probabilities(this_tokens, model, 0, this_pred_pos, verifying = True)\n",
    "\n",
    "    # For the ground truth idx? How to test this? Do it by cross-checking it with the probs in the \"most likely\".\n",
    "    result_df = report_mask_words(probs, test_sentence, tok)\n",
    "\n",
    "    test_word = 'moon' if model_name == 'bert' else 'Ġstore'\n",
    "    ground_truth_token = tok.convert_tokens_to_ids(test_word)\n",
    "    \n",
    "    print(f'Ground truth probability, manual extract, for the word {test_word}: {probs[ground_truth_token]}')\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "bert_df = test_bertlike_model_probs('bert')\n",
    "print(bert_df)\n",
    "\n",
    "#bart_df = test_bertlike_model_probs('bart')\n",
    "#print(bart_df)\n",
    "\n",
    "#gpt2_df = test_gpt2_model_probs()\n",
    "#print(gpt2_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 checks\n",
      "['<|endoftext|>']\n",
      "['<|endoftext|>', 'It']\n",
      "['<|endoftext|>', 'It', \"'s\"]\n",
      "['<|endoftext|>', 'It', \"'s\", 'Ġtime']\n",
      "['<|endoftext|>', 'It', \"'s\", 'Ġtime', 'Ġto']\n",
      "['<|endoftext|>', 'It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo']\n",
      "['<|endoftext|>', 'It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo', 'Ġto']\n",
      "['<|endoftext|>', 'It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo', 'Ġto', 'Ġthe']\n",
      "['<|endoftext|>', 'It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo', 'Ġto', 'Ġthe', 'Ġstore']\n",
      "['<|endoftext|>', 'It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo', 'Ġto', 'Ġthe', 'Ġstore', '.']\n",
      "\n",
      "BERT checks\n",
      "['[CLS]', '[MASK]', \"'\", 's', 'time', 'to', 'go', 'to', 'the', 'store', '.', '[SEP]']\n",
      "['[CLS]', 'it', '[MASK]', 's', 'time', 'to', 'go', 'to', 'the', 'store', '.', '[SEP]']\n",
      "['[CLS]', 'it', \"'\", '[MASK]', 'time', 'to', 'go', 'to', 'the', 'store', '.', '[SEP]']\n",
      "['[CLS]', 'it', \"'\", 's', '[MASK]', 'to', 'go', 'to', 'the', 'store', '.', '[SEP]']\n",
      "['[CLS]', 'it', \"'\", 's', 'time', '[MASK]', 'go', 'to', 'the', 'store', '.', '[SEP]']\n",
      "['[CLS]', 'it', \"'\", 's', 'time', 'to', '[MASK]', 'to', 'the', 'store', '.', '[SEP]']\n",
      "['[CLS]', 'it', \"'\", 's', 'time', 'to', 'go', '[MASK]', 'the', 'store', '.', '[SEP]']\n",
      "['[CLS]', 'it', \"'\", 's', 'time', 'to', 'go', 'to', '[MASK]', 'store', '.', '[SEP]']\n",
      "['[CLS]', 'it', \"'\", 's', 'time', 'to', 'go', 'to', 'the', '[MASK]', '.', '[SEP]']\n",
      "['[CLS]', 'it', \"'\", 's', 'time', 'to', 'go', 'to', 'the', 'store', '[MASK]', '[SEP]']\n",
      "['[CLS]', 'it', \"'\", 's', 'time', 'to', 'go', 'to', 'the', 'store', '.', '[MASK]']\n",
      "\n",
      "BART checks\n",
      "['<s>', '<mask>', \"'s\", 'Ġtime', 'Ġto', 'Ġgo', 'Ġto', 'Ġthe', 'Ġstore', '.', '</s>']\n",
      "['<s>', 'It', '<mask>', 'Ġtime', 'Ġto', 'Ġgo', 'Ġto', 'Ġthe', 'Ġstore', '.', '</s>']\n",
      "['<s>', 'It', \"'s\", '<mask>', 'Ġto', 'Ġgo', 'Ġto', 'Ġthe', 'Ġstore', '.', '</s>']\n",
      "['<s>', 'It', \"'s\", 'Ġtime', '<mask>', 'Ġgo', 'Ġto', 'Ġthe', 'Ġstore', '.', '</s>']\n",
      "['<s>', 'It', \"'s\", 'Ġtime', 'Ġto', '<mask>', 'Ġto', 'Ġthe', 'Ġstore', '.', '</s>']\n",
      "['<s>', 'It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo', '<mask>', 'Ġthe', 'Ġstore', '.', '</s>']\n",
      "['<s>', 'It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo', 'Ġto', '<mask>', 'Ġstore', '.', '</s>']\n",
      "['<s>', 'It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo', 'Ġto', 'Ġthe', '<mask>', '.', '</s>']\n",
      "['<s>', 'It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo', 'Ġto', 'Ġthe', 'Ġstore', '<mask>', '</s>']\n",
      "['<s>', 'It', \"'s\", 'Ġtime', 'Ġto', 'Ġgo', 'Ġto', 'Ġthe', 'Ġstore', '.', '<mask>']\n"
     ]
    }
   ],
   "source": [
    "## Testing the model prefixes functions\n",
    "\n",
    "\n",
    "from new_models import model_prefixes\n",
    "\n",
    "import importlib\n",
    "importlib.reload(model_prefixes)\n",
    "\n",
    "\n",
    "test_sentence = \"It's time to go to the store.\"\n",
    "    \n",
    "def test_gpt2_prefixes(s):\n",
    "    tok = tokenizers['gpt2']\n",
    "    prefixes, _ = model_prefixes.get_gpt2_prefixes(s, tok)\n",
    "    for p in prefixes:\n",
    "        print(tok.convert_ids_to_tokens(p))\n",
    "        \n",
    "def test_bertlike_prefixes(s, model_name):\n",
    "    \n",
    "    tok = tokenizers[model_name]\n",
    "    mask_func = model_prefixes.get_bertlike_mask_func(tok)\n",
    "    \n",
    "    prefixes, _ = mask_func(s, tok)\n",
    "    for p in prefixes:\n",
    "        print(tok.convert_ids_to_tokens(p))\n",
    "\n",
    "print('GPT2 checks')\n",
    "test_gpt2_prefixes(test_sentence)\n",
    "print('\\nBERT checks')\n",
    "test_bertlike_prefixes(test_sentence, 'bert')\n",
    "print('\\nBART checks')\n",
    "test_bertlike_prefixes(test_sentence, 'bart')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telephone-env-3",
   "language": "python",
   "name": "telephone-env-3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
