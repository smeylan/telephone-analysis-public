{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for verifications/dataset analyses from project start approximately to Data Prep Logistic postprocessing (NaN alignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertForMaskedLM, BertTokenizer, BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "import os\n",
    "from os.path import join, exists\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "os.chdir('..')\n",
    "\n",
    "from new_models import model_prefixes, align_prep_words\n",
    "\n",
    "from new_models import in_progress\n",
    "from new_models.in_progress import sub_analysis\n",
    "\n",
    "import importlib\n",
    "importlib.reload(model_prefixes)\n",
    "\n",
    "os.chdir(curr_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "models = {\n",
    "    'gpt2': GPT2LMHeadModel.from_pretrained('gpt2'),\n",
    "    'bert': BertForMaskedLM.from_pretrained('bert-large-uncased-whole-word-masking'),\n",
    "    'bart': BartForConditionalGeneration.from_pretrained('facebook/bart-base'),\n",
    "    \n",
    "}\n",
    "\n",
    "tokenizers = {\n",
    "    'gpt2': GPT2Tokenizer.from_pretrained('gpt2'),\n",
    "    'bert': BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking\"),\n",
    "    'bart': BartTokenizer.from_pretrained(\"facebook/bart-base\"),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "For model: bert, length: 3193\n",
      "gpt2_normal\n",
      "For model: gpt2_normal, length: 3193\n",
      "bart\n",
      "For model: bart, length: 3193\n",
      "gpt2_medium\n",
      "For model: gpt2_medium, length: 3193\n"
     ]
    }
   ],
   "source": [
    "from new_models import prep_probs\n",
    "\n",
    "def load_pred_results(RESULTS_FOLDER):\n",
    "    temp_results = {}\n",
    "    for model_name in ['bert', 'gpt2_normal', 'bart', 'gpt2_medium']:\n",
    "        print(model_name)\n",
    "        temp_results[model_name] = prep_probs.load_word_scores(model_name, RESULTS_FOLDER)\n",
    "        print(f'For model: {model_name}, length: {len(temp_results[model_name])}')\n",
    "    return temp_results\n",
    "\n",
    "\n",
    "PROB_DF_PATH = '/home/nwong/chompsky/serial_chain/telephone-analysis-public/intermediate_results/new_models_probs'\n",
    "prob_results = load_pred_results(PROB_DF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checks and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Each', 'Ġnon', 'fiction', 'Ġbook']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "# Checking whether the postprocessed logistic probabilities match the original probabilities?\n",
    "\n",
    "DATA_PREP_FOLDER = '/home/nwong/chompsky/serial_chain/telephone-analysis-public/intermediate_results/data_prep_logistic' # What is meant by this path?\n",
    "\n",
    "model_names = [filename.split('logistic/')[1].split('_predictions.txt')[0]\n",
    "               for filename in glob.glob(DATA_PREP_FOLDER+'/*')]\n",
    "\n",
    "lm = {}\n",
    "\n",
    "for lm_name in model_names:\n",
    "    raw_scores_path = join(DATA_PREP_FOLDER, f\"{lm_name}_predictions.txt\")\n",
    "    # 3/27: https://stackoverflow.com/questions/27745500/how-to-save-a-list-to-a-file-and-read-it-as-a-list-type\n",
    "    with open(raw_scores_path, 'rb') as f:\n",
    "        raw_scores = pickle.load(f)\n",
    "        lm[lm_name] = raw_scores\n",
    "        \n",
    "BERT_case = 'A dietitian goes'\n",
    "GPT_case = 'Each nonfiction book'\n",
    "\n",
    "tokenizers['bert'].tokenize(BERT_case)\n",
    "tokenizers['gpt2'].tokenize(GPT_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austin</td>\n",
       "      <td>-4.403163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ġclosed</td>\n",
       "      <td>-4.549038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ġthe</td>\n",
       "      <td>-0.950157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ġdoor</td>\n",
       "      <td>-1.287342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ġbehind</td>\n",
       "      <td>-2.303025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ġhim</td>\n",
       "      <td>-0.651266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ġon</td>\n",
       "      <td>-1.327117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ġthe</td>\n",
       "      <td>-0.748366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ġboat</td>\n",
       "      <td>-3.311556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word      prob\n",
       "0   Austin -4.403163\n",
       "1  Ġclosed -4.549038\n",
       "2     Ġthe -0.950157\n",
       "3    Ġdoor -1.287342\n",
       "4  Ġbehind -2.303025\n",
       "5     Ġhim -0.651266\n",
       "6      Ġon -1.327117\n",
       "7     Ġthe -0.748366\n",
       "8    Ġboat -3.311556"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word_list = [df['word'].values.tolist() for df in lm['bnc_unigram']]\n",
    "result = align_prep_words.align_model_word_dfs(lm['gpt2_normal_scores'][:2], tokenizers['gpt2'], word_list[:21])\n",
    "\n",
    "# Verified that this is as expected, 5/31/21\n",
    "\n",
    "align_prep_words.process_nan_single_df(prob_results['gpt2_normal'][1260], tokenizers['gpt2'], word_list[1260])\n",
    "prob_results['gpt2_normal'][1260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       word      prob\n",
      "0      Each -3.570354\n",
      "1      Ġnon -3.581758\n",
      "2   fiction -1.928502\n",
      "3     Ġbook -0.357789\n",
      "4      Ġhas -1.236761\n",
      "5        Ġa -0.538658\n",
      "6     Ġcall -4.164424\n",
      "7   Ġnumber -3.439832\n",
      "8       Ġon -1.361023\n",
      "9      Ġits -1.021375\n",
      "10   Ġspine -1.287340\n",
      "11       Ġ. -3.513690\n",
      "entire idx 1 to collapse 1\n",
      "entire word nonfiction\n",
      "to collapse idx non\n",
      "\t Next collapse: 3\n",
      "\n",
      "       prob        word\n",
      "0 -3.570354        each\n",
      "1       NaN  nonfiction\n",
      "2 -0.357789        book\n",
      "3 -1.236761         has\n",
      "4 -0.538658           a\n",
      "5 -4.164424        call\n",
      "6 -3.439832      number\n",
      "7 -1.361023          on\n",
      "8 -1.021375         its\n",
      "9 -1.287340       spine\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def test_bert_dietitian():\n",
    "    test_idx = 18\n",
    "\n",
    "    orig_score = lm['bert_scores'][test_idx]\n",
    "\n",
    "    print(orig_score)\n",
    "\n",
    "    reference_sentence = 'a dietitian goes to college for at least four years'\n",
    "    result = process_nan_single_df(lm['bert_scores'][test_idx], tokenizers['bert'], reference_sentence)\n",
    "    \n",
    "    print(result)\n",
    "\n",
    "def test_gpt2_nonfiction():\n",
    "    \n",
    "    test_idx = 0\n",
    "\n",
    "    orig_score = lm['gpt2_normal_scores'][test_idx]\n",
    "\n",
    "    print(orig_score)\n",
    "\n",
    "    reference_sentence = 'each nonfiction book has a call number on its spine'\n",
    "    result = process_nan_single_df(lm['gpt2_normal_scores'][test_idx], tokenizers['gpt2'], reference_sentence)\n",
    "    \n",
    "    print(result)\n",
    "    \n",
    "    \n",
    "test_gpt2_nonfiction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>-0.124584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>diet</td>\n",
       "      <td>-0.714438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>##itia</td>\n",
       "      <td>-0.188612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>##n</td>\n",
       "      <td>-0.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goes</td>\n",
       "      <td>-0.189202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>-0.074309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>college</td>\n",
       "      <td>-1.313672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>for</td>\n",
       "      <td>-0.032421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>at</td>\n",
       "      <td>-0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>least</td>\n",
       "      <td>-0.000926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>four</td>\n",
       "      <td>-0.853766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>years</td>\n",
       "      <td>-0.015433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>.</td>\n",
       "      <td>-0.003171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word      prob\n",
       "0         a -0.124584\n",
       "1      diet -0.714438\n",
       "2    ##itia -0.188612\n",
       "3       ##n -0.046200\n",
       "4      goes -0.189202\n",
       "5        to -0.074309\n",
       "6   college -1.313672\n",
       "7       for -0.032421\n",
       "8        at -0.000133\n",
       "9     least -0.000926\n",
       "10     four -0.853766\n",
       "11    years -0.015433\n",
       "12        . -0.003171"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_results['bert'][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Each</td>\n",
       "      <td>-3.225354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ġnon</td>\n",
       "      <td>-3.874274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ġfiction</td>\n",
       "      <td>-5.236221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ġbook</td>\n",
       "      <td>-0.472252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ġhas</td>\n",
       "      <td>-1.158080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ġa</td>\n",
       "      <td>-0.485143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ġcall</td>\n",
       "      <td>-3.748181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ġnumber</td>\n",
       "      <td>-2.511031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ġin</td>\n",
       "      <td>-1.469794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ġits</td>\n",
       "      <td>-1.263469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ġspine</td>\n",
       "      <td>-2.077428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ġ.</td>\n",
       "      <td>-3.149267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word      prob\n",
       "0       Each -3.225354\n",
       "1       Ġnon -3.874274\n",
       "2   Ġfiction -5.236221\n",
       "3      Ġbook -0.472252\n",
       "4       Ġhas -1.158080\n",
       "5         Ġa -0.485143\n",
       "6      Ġcall -3.748181\n",
       "7    Ġnumber -2.511031\n",
       "8        Ġin -1.469794\n",
       "9       Ġits -1.263469\n",
       "10    Ġspine -2.077428\n",
       "11        Ġ. -3.149267"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_results['gpt2_medium'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_find_next_word_bert():\n",
    "    \n",
    "    reference_sentence = 'a dietitian goes to college for at least four years'\n",
    "    \n",
    "    tok = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n",
    "    ttokens = [filter_symbols(t) for t in tok.tokenize(reference_sentence)]\n",
    "    tidx = find_next_word_loc(1, ttokens, 'dietitian')\n",
    "    \n",
    "    assert tidx == 4 \n",
    "    \n",
    "def test_find_next_word_gpt2():\n",
    "    \n",
    "    reference_sentence = 'each nonfiction book has a call number on its spine'\n",
    "    tok = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    ttokens = [filter_symbols(t) for t in tok.tokenize(reference_sentence)]\n",
    "    tidx = find_next_word_loc(1, ttokens, 'nonfiction')\n",
    "    \n",
    "    assert tidx == 3\n",
    "\n",
    "test_find_next_word_bert()\n",
    "test_find_next_word_gpt2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 0\n",
      "It 0.00911561120301485\n",
      "'s 0.4054446220397949\n",
      "Ġtime 0.034498848021030426\n",
      "Ġto 0.548720121383667\n",
      "Ġgo 0.015521024353802204\n",
      "Ġto 0.08927261829376221\n",
      "Ġthe 0.1976589858531952\n",
      "Ġstore 0.019540343433618546\n",
      "Ġ. 0.0002333719312446192\n",
      "<|endoftext|> 0.004117126576602459\n",
      "Walk 3.4847416827687994e-05\n",
      "Ġslowly 0.00015021645231172442\n",
      "Ġto 0.03525398299098015\n",
      "Ġthe 0.47555652260780334\n",
      "Ġgolden 4.06051694881171e-05\n",
      "Ġstair 0.006223683711141348\n",
      "Ġ. 3.163226574542932e-05\n",
      "<|endoftext|> 0.0008614695398136973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[            word      prob\n",
       " 0             It  0.009116\n",
       " 1             's  0.405445\n",
       " 2          Ġtime  0.034499\n",
       " 3            Ġto  0.548720\n",
       " 4            Ġgo  0.015521\n",
       " 5            Ġto  0.089273\n",
       " 6           Ġthe  0.197659\n",
       " 7         Ġstore  0.019540\n",
       " 8             Ġ.  0.000233\n",
       " 9  <|endoftext|>  0.004117,             word      prob\n",
       " 0           Walk  0.000035\n",
       " 1        Ġslowly  0.000150\n",
       " 2            Ġto  0.035254\n",
       " 3           Ġthe  0.475557\n",
       " 4        Ġgolden  0.000041\n",
       " 5         Ġstair  0.006224\n",
       " 6             Ġ.  0.000032\n",
       " 7  <|endoftext|>  0.000861]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import new_models\n",
    "from new_models import model_score_funcs\n",
    "\n",
    "\n",
    "import importlib\n",
    "importlib.reload(model_score_funcs)\n",
    "\n",
    "\n",
    "inputs = [\n",
    "    \"it's time to go to the store\",\n",
    "    \"walk slowly to the golden stair\"\n",
    "]\n",
    "\n",
    "model_score_funcs.get_gpt2_scores(inputs)\n",
    "#model_score_funcs.get_bert_scores(inputs)\n",
    "#model_score_funcs.get_bart_scores(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting most likely tokens to complete 'it's time to go to the [MASK]' in descending order\n",
      "Ground truth probability, manual extract, for the word moon: 0.008538895286619663\n",
      "        Word  Score value\n",
      "0   hospital      0.06366\n",
      "1      beach      0.03984\n",
      "2   cemetery      0.03216\n",
      "3   bathroom      0.02819\n",
      "4      party      0.02643\n",
      "5     movies      0.02543\n",
      "6    meeting      0.02479\n",
      "7    airport      0.01794\n",
      "8     office      0.01390\n",
      "9    library      0.01348\n",
      "10       zoo      0.01343\n",
      "11    church      0.01283\n",
      "12    doctor      0.01203\n",
      "13    rescue      0.01107\n",
      "14   funeral      0.01032\n",
      "15      city      0.01001\n",
      "16      park      0.00918\n",
      "17   station      0.00917\n",
      "18      moon      0.00854\n",
      "19    museum      0.00797\n",
      "Reporting most likely tokens to complete 'it's time to go to the <mask>' in descending order\n",
      "Ground truth probability, manual extract, for the word Ġstore: 0.015764907002449036\n",
      "         Word  Score value\n",
      "0        Ġgym      0.16189\n",
      "1   Ġbathroom      0.06579\n",
      "2     Ġmovies      0.02751\n",
      "3    Ġdentist      0.02241\n",
      "4     Ġtoilet      0.02213\n",
      "5      Ġbeach      0.02174\n",
      "6     Ġoffice      0.01804\n",
      "7    Ġlibrary      0.01678\n",
      "8     Ġdoctor      0.01655\n",
      "9      Ġstore      0.01576\n",
      "10      Ġbank      0.01436\n",
      "11      Ġpark      0.01356\n",
      "12     Ġpolls      0.01153\n",
      "13      Ġdump      0.01143\n",
      "14      Ġnext      0.00968\n",
      "15  Ġhospital      0.00950\n",
      "16      Ġgame      0.00863\n",
      "17      Ġpool      0.00821\n",
      "18       Ġbar      0.00707\n",
      "19   Ġkitchen      0.00678\n",
      "Note that this outputs softmax of the last word before the punctuation.\n",
      "Reporting most likely tokens to complete '<|endoftext|>it's time to go to the<|endoftext|>' in descending order\n",
      "Ground truth probability, manual extract, for the word Ġbathroom: 0.02889152243733406\n",
      "         Word  Score value\n",
      "0      Ġpolls      0.03010\n",
      "1   Ġbathroom      0.02889\n",
      "2       Ġnext      0.02142\n",
      "3        Ġgym      0.02135\n",
      "4     Ġmovies      0.02033\n",
      "5      Ġbeach      0.01769\n",
      "6      Ġstore      0.01736\n",
      "7       Ġmoon      0.01464\n",
      "8       Ġmall      0.01384\n",
      "9    Ġgrocery      0.00837\n",
      "10    Ġtoilet      0.00803\n",
      "11       Ġend      0.00764\n",
      "12  Ġhospital      0.00669\n",
      "13    Ġoffice      0.00583\n",
      "14   Ġlibrary      0.00575\n",
      "15     Ġwoods      0.00575\n",
      "16    Ġdoctor      0.00557\n",
      "17   Ġairport      0.00545\n",
      "18     Ġgrave      0.00537\n",
      "19   Ġtrouble      0.00510\n"
     ]
    }
   ],
   "source": [
    "## Testing the next model probabilities\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from new_models import model_score_utils\n",
    "\n",
    "importlib.reload(model_score_utils)\n",
    "\n",
    "\n",
    "def report_mask_words(scores, sentence, tokenizer):\n",
    "    \"\"\"\n",
    "    raw_scores = a (vocabulary,) tensor of selected softmax values for a pre-selected position.\n",
    "    mask_idx, the position to select for analysis.\n",
    "    \n",
    "    sentence = the prefix to do the prediction on\n",
    "    tokenizer = BERT/BART tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # It should intake the raw scores itself.\n",
    "    score_vals, word_idxs = torch.sort(scores, descending = True)\n",
    "    words = tokenizer.convert_ids_to_tokens(word_idxs)\n",
    "\n",
    "    print(f\"Reporting most likely tokens to complete '{sentence}' in descending order\")\n",
    "\n",
    "    num_report = 20\n",
    "\n",
    "    score_df = pd.DataFrame.from_dict({\n",
    "      'Word': words,\n",
    "      'Score value': list(map(lambda x : round(x, 5), score_vals.numpy().tolist()))\n",
    "      })\n",
    "\n",
    "    return score_df[:num_report]\n",
    "\n",
    "\n",
    "def test_gpt2_model_probs():\n",
    "    \n",
    "    \n",
    "    print('Note that this outputs softmax of the last word before the punctuation.')\n",
    "    \n",
    "    model = models['gpt2']; tok = tokenizers['gpt2']\n",
    "    test_sentence = \"it's time to go to the\"\n",
    "    # Below line is from the prefix code\n",
    "    test_sentence = f'{tok.bos_token}{test_sentence}{tok.eos_token}'\n",
    "    \n",
    "    this_tokens = tok.encode(test_sentence)\n",
    "    \n",
    "    # The 0 is a filler index.\n",
    "    this_pred_pos = len(this_tokens) - 2\n",
    "    _, probs, _ = model_score_utils.get_model_probabilities(this_tokens, model, 0, this_pred_pos, verifying = True)\n",
    "    result_df = report_mask_words(probs, test_sentence, tok)\n",
    "    \n",
    "    test_word = 'Ġbathroom'\n",
    "    ground_truth_token = tok.convert_tokens_to_ids(test_word)\n",
    "    \n",
    "    print(f'Ground truth probability, manual extract, for the word {test_word}: {probs[ground_truth_token]}')\n",
    "    \n",
    "    return result_df\n",
    "    \n",
    "    \n",
    "def test_bertlike_model_probs(model_name):\n",
    "        \n",
    "    model = models[model_name]; tok = tokenizers[model_name]\n",
    "    \n",
    "    test_sentence = f\"it's time to go to the {tok.mask_token}\"\n",
    "    this_tokens = tok.encode(test_sentence)\n",
    "    this_pred_pos = len(this_tokens) - 2\n",
    "    \n",
    "    # The 0 is a filler index.\n",
    "    prob_at_ground_truth, probs, _ = model_score_utils.get_model_probabilities(this_tokens, model, 0, this_pred_pos, verifying = True)\n",
    "\n",
    "    # For the ground truth idx? How to test this? Do it by cross-checking it with the probs in the \"most likely\".\n",
    "    result_df = report_mask_words(probs, test_sentence, tok)\n",
    "\n",
    "    test_word = 'moon' if model_name == 'bert' else 'Ġstore'\n",
    "    ground_truth_token = tok.convert_tokens_to_ids(test_word)\n",
    "    \n",
    "    print(f'Ground truth probability, manual extract, for the word {test_word}: {probs[ground_truth_token]}')\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "bert_df = test_bertlike_model_probs('bert')\n",
    "print(bert_df)\n",
    "\n",
    "bart_df = test_bertlike_model_probs('bart')\n",
    "print(bart_df)\n",
    "\n",
    "gpt2_df = test_gpt2_model_probs()\n",
    "print(gpt2_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is updated\n",
      "[ M A S K ]\n",
      "Reporting most likely tokens to complete 'each nonfiction book had a call number on its spine' in descending order\n",
      "          Word  Score value\n",
      "0        first      0.29029\n",
      "1         last      0.15535\n",
      "2        final      0.07602\n",
      "3       second      0.05278\n",
      "4          top      0.03681\n",
      "5        third      0.02572\n",
      "6         next      0.01916\n",
      "7        fifth      0.01754\n",
      "8       bottom      0.01634\n",
      "9      seventh      0.01319\n",
      "10      fourth      0.01292\n",
      "11        main      0.01015\n",
      "12       front      0.00830\n",
      "13      eighth      0.00811\n",
      "14      spiral      0.00801\n",
      "15      lowest      0.00776\n",
      "16        back      0.00671\n",
      "17       tenth      0.00656\n",
      "18  thirteenth      0.00527\n",
      "19       ninth      0.00514\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tok = tokenizers['bert']\n",
    "\n",
    "# {tok.mask_token} nonfiction book had a call number on its spine\n",
    "this_test = f'walk slowly to the {tok.mask_token} stair'\n",
    "this_tokens = tokenizers['bert'].encode(this_test)\n",
    "\n",
    "pred_idx = 5\n",
    "prob_at_ground_truth, probs = model_score_utils.get_model_probabilities(this_tokens, models['bert'], 0, pred_idx, verifying = True)\n",
    "\n",
    "print(tok.decode(this_tokens[pred_idx]))\n",
    "\n",
    "# For the ground truth idx? How to test this? Do it by cross-checking it with the probs in the \"most likely\".\n",
    "result_df = report_mask_words(probs, test_sentence, tokenizers['bert'])\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** OLD TESTS ************\n",
      "GPT2 checks\n",
      "['<|endoftext|>']\n",
      "['<|endoftext|>', 'each']\n",
      "['<|endoftext|>', 'each', 'Ġnon']\n",
      "['<|endoftext|>', 'each', 'Ġnon', 'fiction']\n",
      "['<|endoftext|>', 'each', 'Ġnon', 'fiction', 'Ġbook']\n",
      "['<|endoftext|>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad']\n",
      "['<|endoftext|>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', 'Ġa']\n",
      "['<|endoftext|>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', 'Ġa', 'Ġcall']\n",
      "['<|endoftext|>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', 'Ġa', 'Ġcall', 'Ġnumber']\n",
      "['<|endoftext|>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', 'Ġa', 'Ġcall', 'Ġnumber', 'Ġon']\n",
      "['<|endoftext|>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', 'Ġa', 'Ġcall', 'Ġnumber', 'Ġon', 'Ġits']\n",
      "['<|endoftext|>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', 'Ġa', 'Ġcall', 'Ġnumber', 'Ġon', 'Ġits', 'Ġspine']\n",
      "\n",
      "BERT checks\n",
      "['[CLS]', '[MASK]', 'nonfiction', 'book', 'had', 'a', 'call', 'number', 'on', 'its', 'spine', '[SEP]']\n",
      "['[CLS]', 'each', '[MASK]', 'book', 'had', 'a', 'call', 'number', 'on', 'its', 'spine', '[SEP]']\n",
      "['[CLS]', 'each', 'nonfiction', '[MASK]', 'had', 'a', 'call', 'number', 'on', 'its', 'spine', '[SEP]']\n",
      "['[CLS]', 'each', 'nonfiction', 'book', '[MASK]', 'a', 'call', 'number', 'on', 'its', 'spine', '[SEP]']\n",
      "['[CLS]', 'each', 'nonfiction', 'book', 'had', '[MASK]', 'call', 'number', 'on', 'its', 'spine', '[SEP]']\n",
      "['[CLS]', 'each', 'nonfiction', 'book', 'had', 'a', '[MASK]', 'number', 'on', 'its', 'spine', '[SEP]']\n",
      "['[CLS]', 'each', 'nonfiction', 'book', 'had', 'a', 'call', '[MASK]', 'on', 'its', 'spine', '[SEP]']\n",
      "['[CLS]', 'each', 'nonfiction', 'book', 'had', 'a', 'call', 'number', '[MASK]', 'its', 'spine', '[SEP]']\n",
      "['[CLS]', 'each', 'nonfiction', 'book', 'had', 'a', 'call', 'number', 'on', '[MASK]', 'spine', '[SEP]']\n",
      "['[CLS]', 'each', 'nonfiction', 'book', 'had', 'a', 'call', 'number', 'on', 'its', '[MASK]', '[SEP]']\n",
      "['[CLS]', 'each', 'nonfiction', 'book', 'had', 'a', 'call', 'number', 'on', 'its', 'spine', '[MASK]']\n",
      "\n",
      "BART checks\n",
      "['<s>', '<mask>', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', 'Ġa', 'Ġcall', 'Ġnumber', 'Ġon', 'Ġits', 'Ġspine', '</s>']\n",
      "['<s>', 'each', '<mask>', 'fiction', 'Ġbook', 'Ġhad', 'Ġa', 'Ġcall', 'Ġnumber', 'Ġon', 'Ġits', 'Ġspine', '</s>']\n",
      "['<s>', 'each', 'Ġnon', '<mask>', 'Ġbook', 'Ġhad', 'Ġa', 'Ġcall', 'Ġnumber', 'Ġon', 'Ġits', 'Ġspine', '</s>']\n",
      "['<s>', 'each', 'Ġnon', 'fiction', '<mask>', 'Ġhad', 'Ġa', 'Ġcall', 'Ġnumber', 'Ġon', 'Ġits', 'Ġspine', '</s>']\n",
      "['<s>', 'each', 'Ġnon', 'fiction', 'Ġbook', '<mask>', 'Ġa', 'Ġcall', 'Ġnumber', 'Ġon', 'Ġits', 'Ġspine', '</s>']\n",
      "['<s>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', '<mask>', 'Ġcall', 'Ġnumber', 'Ġon', 'Ġits', 'Ġspine', '</s>']\n",
      "['<s>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', 'Ġa', '<mask>', 'Ġnumber', 'Ġon', 'Ġits', 'Ġspine', '</s>']\n",
      "['<s>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', 'Ġa', 'Ġcall', '<mask>', 'Ġon', 'Ġits', 'Ġspine', '</s>']\n",
      "['<s>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', 'Ġa', 'Ġcall', 'Ġnumber', '<mask>', 'Ġits', 'Ġspine', '</s>']\n",
      "['<s>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', 'Ġa', 'Ġcall', 'Ġnumber', 'Ġon', '<mask>', 'Ġspine', '</s>']\n",
      "['<s>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', 'Ġa', 'Ġcall', 'Ġnumber', 'Ġon', 'Ġits', '<mask>', '</s>']\n",
      "['<s>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', 'Ġa', 'Ġcall', 'Ġnumber', 'Ġon', 'Ġits', 'Ġspine', '<mask>']\n",
      "\n",
      "******** NEW TESTS ************\n",
      "['<|endoftext|>', 'each', 'Ġnon']\n",
      "['[CLS]', '[MASK]', 'nonfiction', 'book', 'had', 'a', 'call', 'number', 'on', 'its', 'spine', '[SEP]']\n",
      "['<s>', 'each', 'Ġnon', 'fiction', 'Ġbook', 'Ġhad', 'Ġa', '<mask>', 'Ġnumber', 'Ġon', 'Ġits', 'Ġspine', '</s>']\n"
     ]
    }
   ],
   "source": [
    "## Testing the old model prefixes functions (without specifying a given position)\n",
    "\n",
    "test_sentence = \"It's time to go to the store.\"\n",
    "    \n",
    "def test_gpt2_prefixes(s, positions = []):\n",
    "    tok = tokenizers['gpt2']\n",
    "    prefixes, _ = model_prefixes.get_gpt2_prefixes(s, tok, positions)\n",
    "    for p in prefixes:\n",
    "        print(tok.convert_ids_to_tokens(p))\n",
    "        \n",
    "def test_bertlike_prefixes(s, model_name, positions = []):\n",
    "    \n",
    "    tok = tokenizers[model_name]\n",
    "    mask_func = model_prefixes.get_bertlike_mask_func(tok)\n",
    "    \n",
    "    prefixes, _ = mask_func(s, tok, positions)\n",
    "    for p in prefixes:\n",
    "        print(tok.convert_ids_to_tokens(p))\n",
    "\n",
    "test_sentence = \"each nonfiction book had a call number on its spine\"\n",
    "\n",
    "print('******** OLD TESTS ************')\n",
    "print('GPT2 checks')\n",
    "test_gpt2_prefixes(test_sentence)\n",
    "print('\\nBERT checks')\n",
    "test_bertlike_prefixes(test_sentence, 'bert')\n",
    "print('\\nBART checks')\n",
    "test_bertlike_prefixes(test_sentence, 'bart')\n",
    "\n",
    "\n",
    "print('\\n******** NEW TESTS ************')\n",
    "test_gpt2_prefixes(test_sentence, positions = [3])\n",
    "test_bertlike_prefixes(test_sentence, 'bert', positions = [1])\n",
    "test_bertlike_prefixes(test_sentence, 'bart', positions = [7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each nonfiction book has a call number on its spine\n",
      "7.0\n",
      "each non fiction book has a call number in its spine\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "print(this_entry['sentence'])\n",
    "print(this_entry['sCounter'])\n",
    "\n",
    "\n",
    "print(this_entry['response'])\n",
    "print(this_entry['rCounter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "S\n",
      "on\n",
      "in\n",
      "7.0\n",
      "8.0\n",
      "each nonfiction book has a call number on its spine\n",
      "each non fiction book has a call number in its spine\n",
      "call number on\n",
      "call number in\n",
      "on its spine\n",
      "nan\n",
      "0\n",
      "8cf6535ea0ae4addb28f5f90a2b13a7d\n"
     ]
    }
   ],
   "source": [
    "for data in this_entry:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'code', 'sWord', 'rWord', 'sCounter', 'rCounter',\n",
       "       'sentence', 'response', 'sLeftSequence', 'rLeftSequence',\n",
       "       'sRightSequence', 'rRightSequence', 'input_subject', 'output_subject'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substitution_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix generated [CLS] each nonfiction book has a call number [MASK] its spine [SEP]\n",
      "decoding the token on\n",
      "word prob generated 0.9065350890159607\n",
      "prefix generated [CLS] each non fiction book has a call number [MASK] its spine [SEP]\n",
      "decoding the token in\n",
      "word prob generated 0.014622108079493046\n"
     ]
    }
   ],
   "source": [
    "# Checking for correctness of substitution processing\n",
    "\n",
    "WORD_CHANGES_FOLDER = '/home/nwong/chompsky/serial_chain/telephone-analysis-public/intermediate_results/word_changes'\n",
    "\n",
    "substitution_df = pd.read_csv(join(WORD_CHANGES_FOLDER, 'edit_substitutions.csv'))\n",
    "\n",
    "select_idx = 0\n",
    "this_entry = substitution_df.iloc[select_idx]\n",
    "\n",
    "model_name = 'bert'\n",
    "result = sub_analysis.process_substitution_entry(this_entry, *model_score_funcs.get_bert_modules())\n",
    "\n",
    "# What to do here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, get_bert_masks = model_score_funcs.get_bert_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "os.chdir('..')\n",
    "from new_models import model_prefixes, sub_analysis, model_score_funcs, model_score_utils\n",
    "import importlib\n",
    "importlib.reload(model_prefixes)\n",
    "importlib.reload(sub_analysis)\n",
    "importlib.reload(model_score_utils)\n",
    "\n",
    "os.chdir(curr_dir)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from os.path import join, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to to test two positions somehow -- what is a good sanity check?\n",
    "\n",
    "# Need to manually find 'each nonfiction book has a call number on its spine' and then check its probability for \"on\", under the language models.\n",
    "\n",
    "# Or even manually query the words themselves from the softmax?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discarded code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is really, really slow code\n",
    "\n",
    "def prefix_misalignment_check(df_entry, model, tokenizer, prefix_func):\n",
    "    \n",
    "    orig_pos = int(df_entry['sCounter']) # Need to check if these are 0 indexed?\n",
    "    edited_pos = int(df_entry['rCounter'])\n",
    "    \n",
    "    orig_sentence = df_entry['sentence']\n",
    "    edited_sentence = df_entry['response']\n",
    "    \n",
    "    \n",
    "    def process_single_pos(sentence, position):\n",
    "        \n",
    "        this_prefix, _ = prefix_func(sentence, tokenizer, [position + 1]) # Account for no CLS in the original sentence.\n",
    "        this_prefix = [this_str.strip('Ġ') for this_str in tokenizer.convert_ids_to_tokens(this_prefix[0])[1:]]\n",
    "        # Unwrap, omit the CLS\n",
    "        should_prefix = sentence.split()[:position]\n",
    "        \n",
    "        return this_prefix != should_prefix\n",
    "    \n",
    "    misaligned = []\n",
    "    for sent, pos, report in zip([orig_sentence, edited_sentence], [orig_pos, edited_pos], ['orig', 'edited']):\n",
    "        if process_single_pos(sent, pos): \n",
    "            misaligned.append((sent, pos))\n",
    "    \n",
    "    return misaligned\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Older checks and verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_runs\n",
    "\n",
    "agg_all_runs = load_runs.load_runs()\n",
    "prep_all_runs = pd.read_csv('output/all_runs.csv')\n",
    "\n",
    "\n",
    "prep_list = list(prep_all_runs['user_candidate_transcription'])\n",
    "agg_list = list(agg_all_runs['user_candidate_transcription'])\n",
    "\n",
    "# They seem to be the same sentences, but in different orders.\n",
    "\n",
    "print(set(prep_list) ^ set(agg_list))\n",
    "print(prep_list[1:] == agg_list[1:])\n",
    "\n",
    "print(sorted(prep_list) == sorted(agg_list)) # They are just in different orders. So it should be fine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_runs and BERT scores are not aligned?\n",
    "for i, sent_ref in enumerate(lm['bnc_unigram']):\n",
    "    \n",
    "    if i == 0 : continue \n",
    "        \n",
    "    sent_act = lm['bert_scores'][i] \n",
    "    \n",
    "    print(f'reference: Index {i}')\n",
    "    print(f'\\t{list(sent_ref[\"word\"])}')\n",
    "    print('actual')\n",
    "    print(f'\\t{list(sent_act[\"word\"])}')\n",
    "    print('csv')\n",
    "    print(f'\\t{all_entire_word_reference[i]}')\n",
    "    a = input()\n",
    "    if a == 'quit':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(align_prep_words)\n",
    "importlib.reload(prep_probs)\n",
    "\n",
    "word_list = [df['word'].values.tolist() for df in lm['bnc_unigram']]\n",
    "\n",
    "transformer_names = ['gpt2_normal', 'gpt2_medium', 'bert']#, 'bart']\n",
    "tokenizer_names = ['gpt2', 'gpt2', 'bert']#, 'bart']\n",
    "\n",
    "sel_idx = 20\n",
    "for model_name, tokenizer_name in zip(transformer_names, tokenizer_names):\n",
    "\n",
    "    print(tokenizer_name)\n",
    "    print(f'Processing model name: {model_name}')\n",
    "    \n",
    "    raw_scores = prep_probs.load_word_scores(model_name, RESULTS_FOLDER)\n",
    "    lm[f'{model_name}_scores'] = align_prep_words.align_model_word_dfs(raw_scores[:sel_idx],\n",
    "                                                                       tokenizers[tokenizer_name],\n",
    "                                                                       word_list[:sel_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the rerun results and non-rerun ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "For model: bert, length: 3193\n",
      "gpt2_normal\n",
      "For model: gpt2_normal, length: 3193\n",
      "bart\n",
      "For model: bart, length: 3193\n",
      "gpt2_medium\n",
      "For model: gpt2_medium, length: 3193\n",
      "bert\n",
      "For model: bert, length: 2\n",
      "gpt2_normal\n",
      "For model: gpt2_normal, length: 2\n",
      "bart\n",
      "For model: bart, length: 2\n",
      "gpt2_medium\n",
      "For model: gpt2_medium, length: 2\n"
     ]
    }
   ],
   "source": [
    "main_results_path = '/home/nwong/chompsky/serial_chain/telephone-analysis-public/intermediate_results/new_models_probs'\n",
    "rerun_results_path = '/home/nwong/chompsky/serial_chain/telephone-analysis-public/intermediate_results/new_models_probs/verify_only_delete'\n",
    "\n",
    "orig_results = load_pred_results(main_results_path)\n",
    "rerun_results = load_pred_results(rerun_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[         word      prob\n",
       " 0        each -0.326024\n",
       " 1  nonfiction -3.985277\n",
       " 2        book -2.205348\n",
       " 3         has -0.091852\n",
       " 4           a -0.025494\n",
       " 5        call -2.599102\n",
       " 6      number -1.608924\n",
       " 7          on -0.028152\n",
       " 8         its -0.411839\n",
       " 9       spine -2.007116,        word      prob\n",
       " 0      each -0.335973\n",
       " 1       non -2.138003\n",
       " 2   fiction -0.518504\n",
       " 3      book -0.260865\n",
       " 4       has -0.108281\n",
       " 5         a -0.021503\n",
       " 6      call -2.609315\n",
       " 7    number -1.185523\n",
       " 8        in -1.885149\n",
       " 9       its -0.474270\n",
       " 10    spine -2.233993]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerun_results['bert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model bert\n",
      "The original and rerun results match?  True\n",
      "For model gpt2_normal\n",
      "The original and rerun results match?  True\n",
      "For model bart\n",
      "The original and rerun results match?  True\n",
      "For model gpt2_medium\n",
      "The original and rerun results match?  True\n"
     ]
    }
   ],
   "source": [
    "for model in orig_results:\n",
    "    print(f'For model {model}')\n",
    "    print(\"The original and rerun results match? \", all(orig_results[model][i].equals(rerun_results[model][i]) for i in range(2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telephone-env-3",
   "language": "python",
   "name": "telephone-env-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
