{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for GPT-2 scratchwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3193\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dev_csv_path = './dev_lm_sentence_input.csv'\n",
    "\n",
    "all_sentences = pd.read_csv(dev_csv_path)['user_candidate_transcription']\n",
    "print(len(all_sentences))\n",
    "num_select = 2\n",
    "sentences_subset = list(all_sentences.iloc[:num_select])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpt2_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-21ddb0965f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubset_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# The kernel will die... too many sentences stored?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpt2_scores' is not defined"
     ]
    }
   ],
   "source": [
    "subset_scores = gpt2_scores.score_sentences(sentences_subset, model_type = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'bert_prefix_scores' from '/home/nwong/chompsky/serial_chain/telephone-analysis-public/new_models/bert_prefix_scores.py'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "import bert_prefix_scores\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "# 3/11: importlib help: https://stackoverflow.com/questions/1254370/reimport-a-module-in-python-while-interactive\n",
    "\n",
    "import importlib\n",
    "importlib.reload(bert_prefix_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# General sanity checks for trends -- the trends don't completely match up with expectation,\n",
    "#   but the ones with obvious differences (okay sentence structure vs. bad structure) do.\n",
    "sentence_cases = [\n",
    "    'apple toast bring not unsure test coding', # Should have high surprisal\n",
    "    'the person walked down the street', # Low surprisal\n",
    "    'I did not want to go to the library dolphin', # Should have medium surprisal due to last word.\n",
    "    'I would have preferred to eat libraries', # Should have medium surprisal due to last word.\n",
    "    'I coding test passed consequently did yes', # Should have high surprisal\n",
    "    'this sentence should have a low score', # Low surprisal (or medium after observing the results, because \"score\" is ML-specific)\n",
    "    'this sentence should have a paragraph', # Even lower surprisal -- observationally this isn't the case! Which is interesting.\n",
    "]\n",
    "\n",
    "# These results aren't really intuitive.\n",
    "\n",
    "results = bert_prefix_scores.score_sentences(sentence_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sentence apple toast bring not unsure test coding\n",
      "\tThe surprisal per word was 11.624098641531807\n",
      "For sentence the person walked down the street\n",
      "\tThe surprisal per word was 2.7322870890299478\n",
      "For sentence I did not want to go to the library dolphin\n",
      "\tThe surprisal per word was 3.110466194152832\n",
      "For sentence I would have preferred to eat libraries\n",
      "\tThe surprisal per word was 3.859767641339983\n",
      "For sentence I coding test passed consequently did yes\n",
      "\tThe surprisal per word was 10.099221365792411\n",
      "For sentence this sentence should have a low score\n",
      "\tThe surprisal per word was 3.726261411394392\n",
      "For sentence this sentence should have a paragraph\n",
      "\tThe surprisal per word was 4.932441711425781\n"
     ]
    }
   ],
   "source": [
    "for s, score in zip(sentence_cases, results):\n",
    "    print(f'For sentence {s}')\n",
    "    print(f'\\tThe surprisal per word was {score / len(s.split())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## More informal checks of correctness in BERT\n",
    "import torch\n",
    "\n",
    "#2/20: https://huggingface.co/transformers/quickstart.html\n",
    "\n",
    "# Please note that, for consistency with the standard \"It's time to go to the\" check, I use bert-base-uncased here.\n",
    "# But the actual model used for the tests is the word tokenized one.\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('bert-large-uncased-whole-word-masking')\n",
    "model.eval()\n",
    "    \n",
    "#2/20: https://albertauyeung.github.io/2020/06/19/bert-tokenization.html\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_positions_from_encoded():\n",
    "    \n",
    "    #2/20: https://albertauyeung.github.io/2020/06/19/bert-tokenization.html\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n",
    "\n",
    "    sent = \"This is great\"\n",
    "\n",
    "    enc_s, seg_s = bert_prefix_scores.get_encoded_text(sent, tokenizer)\n",
    "    res_tokens, res_segs, res_next_words = bert_prefix_scores.get_positions_from_encoded(enc_s, seg_s, 103)\n",
    "\n",
    "    expected = [\n",
    "        ['[CLS]', '[MASK]', 'is', 'great', '.', '[SEP]'],\n",
    "        ['[CLS]', 'this', '[MASK]', 'great', '.', '[SEP]'],\n",
    "        ['[CLS]', 'this', 'is', '[MASK]', '.', '[SEP]']\n",
    "    ]\n",
    "    \n",
    "    actual = decode_token_list(res_tokens)\n",
    "    assert actual == expected\n",
    "    \n",
    "test_get_positions_from_encoded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3/11: importlib help: https://stackoverflow.com/questions/1254370/reimport-a-module-in-python-while-interactive\n",
    "import importlib\n",
    "importlib.reload(bert_prefix_scores)\n",
    "\n",
    "import new_model_funcs\n",
    "\n",
    "def prefix_predictions_single(this_sentence):\n",
    "\n",
    "    score, this_probs = bert_prefix_scores.get_bert_sentence_score(this_sentence, tokenizer, model, verifying = True )\n",
    "\n",
    "    raw_tokens = tokenizer.tokenize(this_sentence)\n",
    "\n",
    "    words = sentence.split()\n",
    "    for idx in range(0, this_probs.shape[0]):\n",
    "        # Note that need to negate surprisals to treat them like probabilities, as here.\n",
    "        results, _ = bert_prefix_scores.report_mask_words(this_probs[idx], raw_tokens[:idx], tokenizer)\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting most likely tokens to complete '[]' in descending order\n",
      "       Word  Score value\n",
      "0       the      0.18794\n",
      "1         a      0.06843\n",
      "2         \"      0.02393\n",
      "3         “      0.02370\n",
      "4     first      0.01813\n",
      "5         i      0.01780\n",
      "6       and      0.01361\n",
      "7      good      0.01353\n",
      "8      your      0.01345\n",
      "9        my      0.01294\n",
      "10      you      0.01213\n",
      "11        '      0.01207\n",
      "12     this      0.01188\n",
      "13  morning      0.01070\n",
      "14      our      0.01058\n",
      "15     make      0.00959\n",
      "16       we      0.00929\n",
      "17       no      0.00821\n",
      "18     some      0.00721\n",
      "19     warm      0.00624\n",
      "Reporting most likely tokens to complete '['apple']' in descending order\n",
      "     Word  Score value\n",
      "0      to      0.21832\n",
      "1       ,      0.14313\n",
      "2       i      0.12641\n",
      "3     you      0.11488\n",
      "4    will      0.05468\n",
      "5       -      0.02836\n",
      "6       .      0.02442\n",
      "7   would      0.02088\n",
      "8      we      0.01847\n",
      "9     can      0.01621\n",
      "10   they      0.01315\n",
      "11      :      0.01178\n",
      "12    and      0.00888\n",
      "13   must      0.00847\n",
      "14     is      0.00814\n",
      "15  trees      0.00679\n",
      "16    she      0.00638\n",
      "17  could      0.00489\n",
      "18    for      0.00465\n",
      "19      ?      0.00447\n",
      "Reporting most likely tokens to complete '['apple', 'toast']' in descending order\n",
      "          Word  Score value\n",
      "0         cake      0.02185\n",
      "1           no      0.01650\n",
      "2           vs      0.01589\n",
      "3            c      0.01403\n",
      "4     sandwich      0.01246\n",
      "5         soup      0.01145\n",
      "6      cookies      0.01131\n",
      "7       cereal      0.00931\n",
      "8      pudding      0.00858\n",
      "9            n      0.00756\n",
      "10         ltd      0.00745\n",
      "11         etc      0.00688\n",
      "12    biscuits      0.00669\n",
      "13           2      0.00659\n",
      "14      butter      0.00644\n",
      "15     special      0.00643\n",
      "16      sticks      0.00632\n",
      "17        club      0.00594\n",
      "18      crunch      0.00566\n",
      "19  sandwiches      0.00561\n",
      "Reporting most likely tokens to complete '[]' in descending order\n",
      "      Word  Score value\n",
      "0      the      0.23198\n",
      "1        a      0.04642\n",
      "2        \"      0.03231\n",
      "3     your      0.02402\n",
      "4     make      0.02276\n",
      "5        “      0.01961\n",
      "6       my      0.01383\n",
      "7     this      0.01329\n",
      "8    sweet      0.00994\n",
      "9    apple      0.00979\n",
      "10      no      0.00948\n",
      "11     and      0.00782\n",
      "12     our      0.00781\n",
      "13    what      0.00768\n",
      "14    good      0.00713\n",
      "15     eat      0.00704\n",
      "16     his      0.00695\n",
      "17    that      0.00679\n",
      "18    when      0.00647\n",
      "19  little      0.00616\n",
      "Reporting most likely tokens to complete '['apple']' in descending order\n",
      "     Word  Score value\n",
      "0      to      0.21832\n",
      "1       ,      0.14313\n",
      "2       i      0.12641\n",
      "3     you      0.11488\n",
      "4    will      0.05468\n",
      "5       -      0.02836\n",
      "6       .      0.02442\n",
      "7   would      0.02088\n",
      "8      we      0.01847\n",
      "9     can      0.01621\n",
      "10   they      0.01315\n",
      "11      :      0.01178\n",
      "12    and      0.00888\n",
      "13   must      0.00847\n",
      "14     is      0.00814\n",
      "15  trees      0.00679\n",
      "16    she      0.00638\n",
      "17  could      0.00489\n",
      "18    for      0.00465\n",
      "19      ?      0.00447\n",
      "Reporting most likely tokens to complete '['apple', 'pie']' in descending order\n",
      "        Word  Score value\n",
      "0   festival      0.01487\n",
      "1       soup      0.01168\n",
      "2         vs      0.01067\n",
      "3        inc      0.00776\n",
      "4   magazine      0.00756\n",
      "5      sales      0.00740\n",
      "6      music      0.00700\n",
      "7         no      0.00657\n",
      "8        etc      0.00648\n",
      "9        pie      0.00614\n",
      "10     world      0.00596\n",
      "11       ltd      0.00590\n",
      "12    market      0.00589\n",
      "13      wars      0.00547\n",
      "14      club      0.00538\n",
      "15   company      0.00518\n",
      "16     party      0.00517\n",
      "17     sauce      0.00514\n",
      "18   reviews      0.00512\n",
      "19        co      0.00482\n",
      "Reporting most likely tokens to complete '[]' in descending order\n",
      "       Word  Score value\n",
      "0       the      0.18794\n",
      "1         a      0.06843\n",
      "2         \"      0.02393\n",
      "3         “      0.02370\n",
      "4     first      0.01813\n",
      "5         i      0.01780\n",
      "6       and      0.01361\n",
      "7      good      0.01353\n",
      "8      your      0.01345\n",
      "9        my      0.01294\n",
      "10      you      0.01213\n",
      "11        '      0.01207\n",
      "12     this      0.01188\n",
      "13  morning      0.01070\n",
      "14      our      0.01058\n",
      "15     make      0.00959\n",
      "16       we      0.00929\n",
      "17       no      0.00821\n",
      "18     some      0.00721\n",
      "19     warm      0.00624\n",
      "Reporting most likely tokens to complete '['apple']' in descending order\n",
      "     Word  Score value\n",
      "0      to      0.21832\n",
      "1       ,      0.14313\n",
      "2       i      0.12641\n",
      "3     you      0.11488\n",
      "4    will      0.05468\n",
      "5       -      0.02836\n",
      "6       .      0.02442\n",
      "7   would      0.02088\n",
      "8      we      0.01847\n",
      "9     can      0.01621\n",
      "10   they      0.01315\n",
      "11      :      0.01178\n",
      "12    and      0.00888\n",
      "13   must      0.00847\n",
      "14     is      0.00814\n",
      "15  trees      0.00679\n",
      "16    she      0.00638\n",
      "17  could      0.00489\n",
      "18    for      0.00465\n",
      "19      ?      0.00447\n",
      "Reporting most likely tokens to complete '['apple', 'toast']' in descending order\n",
      "          Word  Score value\n",
      "0         cake      0.02185\n",
      "1           no      0.01650\n",
      "2           vs      0.01589\n",
      "3            c      0.01403\n",
      "4     sandwich      0.01246\n",
      "5         soup      0.01145\n",
      "6      cookies      0.01131\n",
      "7       cereal      0.00931\n",
      "8      pudding      0.00858\n",
      "9            n      0.00756\n",
      "10         ltd      0.00745\n",
      "11         etc      0.00688\n",
      "12    biscuits      0.00669\n",
      "13           2      0.00659\n",
      "14      butter      0.00644\n",
      "15     special      0.00643\n",
      "16      sticks      0.00632\n",
      "17        club      0.00594\n",
      "18      crunch      0.00566\n",
      "19  sandwiches      0.00561\n"
     ]
    }
   ],
   "source": [
    "# Standard sanity check\n",
    "#prefix_predictions_single(\"It's time to go to the store\") \n",
    "\n",
    "# Below: Trying to ensure that strange behavior\n",
    "#     on always choosing the ground truth as the highest next prediction is resolved.\n",
    "\n",
    "# 3/20: This is now fixed, there is no strange behavior.\n",
    "\n",
    "prefix_predictions_single(\"apple toast bring\")\n",
    "prefix_predictions_single(\"apple pie bring\")\n",
    "prefix_predictions_single(\"apple toast bring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python gpt2_tests.py # These might have been broken by BERT development."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telephone-env-3",
   "language": "python",
   "name": "telephone-env-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
