{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import imp\n",
    "import telephone_analysis\n",
    "import srilm\n",
    "import roark\n",
    "import glob\n",
    "import shutil\n",
    "import scipy.stats\n",
    "imp.reload(telephone_analysis)\n",
    "%load_ext rpy2.ipython\n",
    "import rpy2.robjects.lib.ggplot2 as ggplot2\n",
    "import kenlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs  = pd.read_csv('output/all_runs.csv')\n",
    "# run these through each of the language models and merge the results\n",
    "# make a word level table\n",
    "# diff the words to see if they are not present in the next generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_runs.loc[all_runs.user != \"0\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(all_runs.loc[all_runs.user != \"0\"].user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'chain', 'character_levdau', 'check_time',\n",
       "       'condition', 'flag_type', 'gold_candidate_transcription',\n",
       "       'gold_comparison_transcription', 'gold_dist',\n",
       "       ...\n",
       "       'initial_gpt2_normal_probability_rank',\n",
       "       'initial_gpt2_normal_probability_quartile',\n",
       "       'initial_gpt2_medium_probability',\n",
       "       'initial_gpt2_medium_probability_rank',\n",
       "       'initial_gpt2_medium_probability_quartile', 'initial_bert_probability',\n",
       "       'initial_bert_probability_rank', 'initial_bert_probability_quartile',\n",
       "       'thread_id', 'chain_length'],\n",
       "      dtype='object', length=101)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_runs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-Level Language Modeling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index in the LM corresponds to the response: user_candidate_transcription. 3192: last input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc_knn_lm = srilm.LM(\"LMs/BNC_merged.LM\", lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm['bnc_unigram'] =  [telephone_analysis.getSRILMprob(x, {}, bnc_knn_lm, mode='single_word', unigram=True) for x in all_runs['user_candidate_transcription']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(lm['bnc_unigram']) == all_runs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm['bnc_trigram'] =  [telephone_analysis.getSRILMprob(x, {}, bnc_knn_lm, mode='single_word', unigram = False) for x in all_runs['user_candidate_transcription']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(lm['bnc_trigram']) == all_runs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input contains 3193 sentences\n",
      "Input contains 1982 unique sentences\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/home/stephan/utils/incremental-top-down-parser/temp/0.input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-19b94e634912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m lm['roark_scores'] = roark.parse([str(i) for i in all_runs['user_candidate_transcription']], \n\u001b[1;32m      2\u001b[0m     \u001b[0;34m'/home/stephan/utils/incremental-top-down-parser'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                           numWorkers=24, mode='single_word')\n\u001b[0m",
      "\u001b[0;32m/home/nwong/chompsky/serial_chain/telephone-analysis-public/roark.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(sentences, roark_base_path, numWorkers, mode)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfileIndex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileIndices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mworkerInputPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroark_base_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'temp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkerInputPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfileIndices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfileIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mworkerInputPaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkerInputPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/home/stephan/utils/incremental-top-down-parser/temp/0.input'"
     ]
    }
   ],
   "source": [
    "lm['roark_scores'] = roark.parse([str(i) for i in all_runs['user_candidate_transcription']], \n",
    "    '/home/stephan/utils/incremental-top-down-parser',\n",
    "                          numWorkers=24, mode='single_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roark_scores.columns = ['WSJ_Roark_'+x.replace(' ','.') for x in roark_scores.columns]\n",
    "assert len(lm['roark_scores']) == all_runs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm['big_lm_scores'] = telephone_analysis.getBigLMscores(all_runs['user_candidate_transcription'], 'big_lm',\n",
    "    'big_lm_cache', colname='BigLM_probability', lm_1b_dir='/home/stephan/python/lm_1b', mode='single_word')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(lm['big_lm_scores']) == all_runs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = kenlm.Model('LMs/deepspeech_5gram.binary')\n",
    "lm['kenlm_scores'] = all_runs['kenlm_probability'] = [telephone_analysis.getKenLMProb(x, m, mode='single_word') for x in all_runs['user_candidate_transcription']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(lm['kenlm_scores']) == all_runs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: all sentences in all models must have the same number of tokens in the dataframe\n",
    "languageModelNames = ['bnc_unigram', 'bnc_trigram', 'roark_scores', 'big_lm_scores', 'kenlm_scores']\n",
    "for i in range(all_runs.shape[0]):\n",
    "    sentences = [lm[x][i] for x in languageModelNames]\n",
    "    try:\n",
    "        numWords = [x.shape[0] for x in sentences]\n",
    "    except:\n",
    "        print('Problem counting words')\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "    if not np.allclose(numWords[1:len(numWords)], numWords[0]):\n",
    "        print('Different number of words')\n",
    "        import pdb\n",
    "        pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_store = []\n",
    "for i in range(all_runs.shape[0]):\n",
    "    # need to rename what is coming out of each language model\n",
    "    word_store = [] \n",
    "    for languageModelName in languageModelNames:\n",
    "        df = lm[languageModelName][i].copy()\n",
    "        try:\n",
    "            df.columns = [languageModelName+'_'+x for x in df.columns]        \n",
    "        except:\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "        word_store.append(df)\n",
    "    lms_combined = pd.concat(word_store, axis =1)\n",
    "    lms_combined['sCounter'] = range(lms_combined.shape[0])\n",
    "    lms_combined['sentence_index'] = i\n",
    "    sentence_store.append(lms_combined)\n",
    "wdf = pd.concat(sentence_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf['word'] = wdf[u'bnc_unigram_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each record is a 'produced' word (user_candidate_transcription) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Deleted and Changed Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for generation n, get the indices of all words that have changed in n+1\n",
    "# have changed: no longer appear? doesn't handle transpositions\n",
    "# borrowed the function from the old version of telephone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs.loc[all_runs.user == \"0\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a 2-column subset of all_trials that we can put into R\n",
    "input_output = all_runs[['gold_candidate_transcription','user_candidate_transcription','user',\n",
    "                         'upstream_subject_id']]\n",
    "#gold_candidate_transcription is what a participant heard\n",
    "#user_candidate_transcription is what the participant produced\n",
    "\n",
    "# remove the intitial sentences -- these are represnted as input for the first participant\n",
    "%R -i input_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "source('getWordLevenshteinDistance.R')\n",
    "print(paste(nrow(input_output), 'sentences'))\n",
    "names(input_output) = c('input','output','output_subject','input_subject')  \n",
    "input_output$input = tolower(as.character(input_output$input))\n",
    "input_output$output = tolower(as.character(input_output$output))\n",
    "\n",
    "computeEditTable = function(s,r,input_subject, output_subject){\n",
    "    if (s == 'none'){\n",
    "        # this is an initial sentence, return NA\n",
    "        return(NA)\n",
    "    } else {\n",
    "        et = getReducedEditTable(s,r)\n",
    "        # for python compatibility, use 0-indices\n",
    "        et$sCounter = et$sCounter - 1\n",
    "        et$rCounter = et$rCounter - 1\n",
    "        et$input_subject = input_subject\n",
    "        et$output_subject = output_subject\n",
    "        return(et)\n",
    "    }\n",
    "}\n",
    "\n",
    "editTables = mapply(computeEditTable, input_output$input, input_output$output,\n",
    "                   input_output$input_subject, input_output$output_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "editTables[[2]] #this should correspond to wdf[,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "print(computeEditTable('this is a', 'this is a mouse', NULL, NULL))\n",
    "print(computeEditTable('this is a mouse', 'this is a', NULL, NULL ))\n",
    "print(computeEditTable('this is a mouse', 'this is a mouse', NULL, NULL))\n",
    "print(computeEditTable('this is a mouse', 'this is a house', NULL, NULL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "getDSMwrtInput = function(sentence_index, editTables){\n",
    "    et = editTables[[sentence_index]]\n",
    "    if (is.na(et)){\n",
    "        return(data.frame(sentence_index, sCounter=NA,code=NA, input_subject=NA, output_subject=NA)) # these are initial sentences\n",
    "    } else{\n",
    "        et = editTables[[sentence_index]]    \n",
    "        et$sentence_index = sentence_index\n",
    "        # here is where insertions are removed so that we can join back with wfds\n",
    "        return(subset(et, !is.na(sCounter))[,c('sentence_index','sCounter','code','input_subject','output_subject')])\n",
    "    }\n",
    "}\n",
    "\n",
    "DSMwrtInput = do.call('rbind', lapply(c(1:length(editTables)), function(i){\n",
    "    dsm = getDSMwrtInput(i, editTables)  \n",
    "    if (!is.na(dsm)){\n",
    "        dsm$sentence_index = dsm$sentence_index - 2 \n",
    "        # -1 because Python indexes from 0\n",
    "        # another -1 to bring the index of edits into alignment with the languag model results     \n",
    "    } \n",
    "    return(dsm)\n",
    "}))   \n",
    "\n",
    "#DSMwrtInput = subset(DSMwrtInput, !is.na(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2.robjects import r, pandas2ri\n",
    "pandas2ri.activate()\n",
    "DSMwrtInput = r['DSMwrtInput']\n",
    "DSMwrtInput.iloc[0:15,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialSentenceIndices = np.array(DSMwrtInput[np.isnan(DSMwrtInput.sCounter)]['sentence_index'].tolist())\n",
    "wdf_initRemoved = wdf[(~wdf['sentence_index'].isin(initialSentenceIndices)) & (wdf['bnc_unigram_word'] != '</s>')]\n",
    "DSMwrtInput_nansRemoved = DSMwrtInput[~np.isnan(DSMwrtInput.sCounter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DSMwrtInput_nansRemoved.shape[0])\n",
    "print(wdf_initRemoved.shape[0])\n",
    "#assert(DSMwrtInput_nansRemoved.shape[0] == wdf_initRemoved.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf_initRemoved[wdf_initRemoved.sentence_index == 3192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSMwrtInput_nansRemoved[DSMwrtInput_nansRemoved.sentence_index == 3192] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_counts = DSMwrtInput_nansRemoved.groupby(['sentence_index']).sCounter.agg(np.size).reset_index()\n",
    "\n",
    "dsm_counts.columns = ['sentence_index', 'dsm_count']\n",
    "wdf_counts = wdf_initRemoved.groupby('sentence_index').sCounter.agg(np.size).reset_index()\n",
    "wdf_counts.columns = ['sentence_index', 'wdf_count']\n",
    "count_check = wdf_counts.merge(dsm_counts, how='outer')\n",
    "count_check # differences in the counts?\n",
    "count_check['difference'] = count_check.wdf_count - count_check.dsm_count\n",
    "count_check.sort_values(by=['difference'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge DSMwrtInput into the word data frame\n",
    "wdfr = wdf_initRemoved.merge(DSMwrtInput_nansRemoved) #sCounter is NaN for input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#where did all my happy data go?\n",
    "wdfr.code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdfr.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge With Word Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdfr['word'] = wdfr['bnc_unigram_word']\n",
    "lexiconch = pd.read_csv('data/lexiconch.csv', index_col=0)\n",
    "print('Number of words before merging with Lexiconch: '+str(wdfr.shape[0]))\n",
    "wdfl = wdfr.merge(lexiconch, how='left')\n",
    "print('Number of words after merging with Lexiconch: '+str(wdfl.shape[0]))\n",
    "' '.join(set(wdfr.word) - set(wdfl.word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexiconch.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yarkoni_pld = pd.read_table('data/pld20.txt', header=None)\n",
    "yarkoni_pld.columns = ['word', 'pld20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of words before merging with Yarkoni PLD: '+str(wdfl.shape[0]))\n",
    "wdfy = wdfl.merge(yarkoni_pld, how='left')\n",
    "print('Number of words after merging with Yarkoni PLD: '+str(wdfy.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex = pd.read_csv('data/subtlex_augmented.csv')\n",
    "subtlex['word'] = subtlex.Word\n",
    "subtlex.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of words before merging with Subtlex: '+str(wdfy.shape[0]))\n",
    "wdfx = wdfy.merge(subtlex[['word','SUBTLCD']], how='left')\n",
    "print('Number of words after merging with Subtlex: '+str(wdfx.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge in Sentence Level Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs['sentence_index'] = range(all_runs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wdfs = wdfr.merge(all_runs, on='sentence_index') # !!! temporarily remove additional predictors\n",
    "wdfs = wdfx.merge(all_runs, on='sentence_index') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdfs.loc[wdfs.sentence_index == 1][['bnc_unigram_word',\n",
    "                                   'user_candidate_transcription',\n",
    "                                    'gold_candidate_transcription',\n",
    "                                    'input_subject',\n",
    "                                    'output_subject'\n",
    "                                   ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(wdfs.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdfs.to_csv('output/wordLevelChanges.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See R notebook, Error Prediction Model - Logistic Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telephone-env-3",
   "language": "python",
   "name": "telephone-env-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
