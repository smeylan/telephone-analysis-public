{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import imp\n",
    "import telephone_analysis\n",
    "import srilm\n",
    "import roark\n",
    "import glob\n",
    "import shutil\n",
    "import scipy.stats\n",
    "imp.reload(telephone_analysis)\n",
    "%load_ext rpy2.ipython\n",
    "import rpy2.robjects.lib.ggplot2 as ggplot2\n",
    "import kenlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs  = pd.read_csv('output/all_runs.csv')\n",
    "# run these through each of the language models and merge the results\n",
    "# make a word level table\n",
    "# diff the words to see if they are not present in the next generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3193, 85)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_runs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'chain', 'character_levdau', 'check_time', 'condition',\n",
       "       'flag_type', 'gold_candidate_transcription',\n",
       "       'gold_comparison_transcription', 'gold_dist', 'length_accept', 'reason',\n",
       "       'run', 'stimulus', 'stimulus_id', 'subject_id', 'target_length',\n",
       "       'upload_time', 'upstream_pointer', 'upstream_subject_id', 'user',\n",
       "       'user_candidate_transcription', 'user_comparison_transcription',\n",
       "       'user_dist', 'user_short', 'word_distances', 'unique_chain_identifier',\n",
       "       'global_chain', 'BNC_KNN_trigramProb', 'BNC_KNN_unigramProb',\n",
       "       'WSJ_gt_unigramProb', 'WSJ_gt_trigramProb', 'WSJ_gt_5gramProb',\n",
       "       'WSJ_Roark_Negative.Log.Probability', 'biglm_probability',\n",
       "       'kenlm_probability', 'bllip_probability', 'bllip_wsj_probability',\n",
       "       'mikolov_wsj_probability', 'length_in_words',\n",
       "       'normalized_biglm_probability',\n",
       "       'normalized_WSJ_Roark_Negative.Log.Probability',\n",
       "       'normalized_BNC_KNN_unigramProb', 'normalized_BNC_KNN_trigramProb',\n",
       "       'normalized_kenlm_probability', 'normalized_bllip_probability',\n",
       "       'normalized_bllip_wsj_probability',\n",
       "       'normalized_mikolov_wsj_probability', 'normalized_WSJ_gt_unigramProb',\n",
       "       'normalized_WSJ_gt_trigramProb', 'normalized_WSJ_gt_5gramProb',\n",
       "       'initial_biglm_probability', 'initial_biglm_probability_rank',\n",
       "       'initial_biglm_probability_quartile',\n",
       "       'initial_WSJ_Roark_Negative.Log.Probability',\n",
       "       'initial_WSJ_Roark_Negative.Log.Probability_rank',\n",
       "       'initial_WSJ_Roark_Negative.Log.Probability_quartile',\n",
       "       'initial_BNC_KNN_unigramProb', 'initial_BNC_KNN_unigramProb_rank',\n",
       "       'initial_BNC_KNN_unigramProb_quartile', 'initial_BNC_KNN_trigramProb',\n",
       "       'initial_BNC_KNN_trigramProb_rank',\n",
       "       'initial_BNC_KNN_trigramProb_quartile', 'initial_kenlm_probability',\n",
       "       'initial_kenlm_probability_rank', 'initial_kenlm_probability_quartile',\n",
       "       'initial_bllip_probability', 'initial_bllip_probability_rank',\n",
       "       'initial_bllip_probability_quartile', 'initial_bllip_wsj_probability',\n",
       "       'initial_bllip_wsj_probability_rank',\n",
       "       'initial_bllip_wsj_probability_quartile',\n",
       "       'initial_mikolov_wsj_probability',\n",
       "       'initial_mikolov_wsj_probability_rank',\n",
       "       'initial_mikolov_wsj_probability_quartile',\n",
       "       'initial_WSJ_gt_unigramProb', 'initial_WSJ_gt_unigramProb_rank',\n",
       "       'initial_WSJ_gt_unigramProb_quartile', 'initial_WSJ_gt_trigramProb',\n",
       "       'initial_WSJ_gt_trigramProb_rank',\n",
       "       'initial_WSJ_gt_trigramProb_quartile', 'initial_WSJ_gt_5gramProb',\n",
       "       'initial_WSJ_gt_5gramProb_rank', 'initial_WSJ_gt_5gramProb_quartile',\n",
       "       'thread_id', 'chain_length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_runs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-Level Language Modeling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index in the LM corresponds to the response: user_candidate_transcription. 3192: last input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc_knn_lm = srilm.LM(\"LMs/BNC_merged.LM\", lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm['bnc_unigram'] =  [telephone_analysis.getSRILMprob(x, {}, bnc_knn_lm, mode='single_word', unigram=True) for x in all_runs['user_candidate_transcription']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(lm['bnc_unigram']) == all_runs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm['bnc_trigram'] =  [telephone_analysis.getSRILMprob(x, {}, bnc_knn_lm, mode='single_word', unigram = False) for x in all_runs['user_candidate_transcription']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(lm['bnc_trigram']) == all_runs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input contains 3193 sentences\n",
      "Input contains 1982 unique sentences\n",
      "Finished external parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/notebooks/telephone-analysis-public/roark.py:235: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  pfix_df = pd.read_table(StringIO('\\n'.join(y)), sep=' ', header=None, names = ['prefix','word']+colnames)\n",
      "/home/stephan/notebooks/telephone-analysis-public/lib/python3.6/site-packages/pandas/io/parsers.py:702: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    }
   ],
   "source": [
    "lm['roark_scores'] = roark.parse([str(i) for i in all_runs['user_candidate_transcription']], \n",
    "    '/home/stephan/utils/incremental-top-down-parser',\n",
    "                          numWorkers=24, mode='single_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roark_scores.columns = ['WSJ_Roark_'+x.replace(' ','.') for x in roark_scores.columns]\n",
    "assert len(lm['roark_scores']) == all_runs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big LM call:\n",
      "cd /home/stephan/python/lm_1b && source bin/activate && bazel-bin/lm_1b/lm_1b_eval --mode eval_sentences --pbtxt data/graph-2016-09-10.pbtxt --vocab_file data/vocab-2016-09-10.txt --eval_dir /home/stephan/notebooks/telephone-analysis-public/big_lm --ckpt 'data/ckpt-*'\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "lm['big_lm_scores'] = telephone_analysis.getBigLMscores(all_runs['user_candidate_transcription'], 'big_lm',\n",
    "    'big_lm_cache', colname='BigLM_probability', lm_1b_dir='/home/stephan/python/lm_1b', mode='single_word')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(lm['big_lm_scores']) == all_runs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = kenlm.Model('LMs/deepspeech_5gram.binary')\n",
    "lm['kenlm_scores'] = all_runs['kenlm_probability'] = [telephone_analysis.getKenLMProb(x, m, mode='single_word') for x in all_runs['user_candidate_transcription']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(lm['kenlm_scores']) == all_runs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: all sentences in all models must have the same number of tokens in the dataframe\n",
    "languageModelNames = ['bnc_unigram', 'bnc_trigram', 'roark_scores', 'big_lm_scores', 'kenlm_scores']\n",
    "for i in range(all_runs.shape[0]):\n",
    "    sentences = [lm[x][i] for x in languageModelNames]\n",
    "    try:\n",
    "        numWords = [x.shape[0] for x in sentences]\n",
    "    except:\n",
    "        print('Problem counting words')\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "    if not np.allclose(numWords[1:len(numWords)], numWords[0]):\n",
    "        print('Different number of words')\n",
    "        import pdb\n",
    "        pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_store = []\n",
    "for i in range(all_runs.shape[0]):\n",
    "    # need to rename what is coming out of each language model\n",
    "    word_store = [] \n",
    "    for languageModelName in languageModelNames:\n",
    "        df = lm[languageModelName][i].copy()\n",
    "        try:\n",
    "            df.columns = [languageModelName+'_'+x for x in df.columns]        \n",
    "        except:\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "        word_store.append(df)\n",
    "    lms_combined = pd.concat(word_store, axis =1)\n",
    "    lms_combined['sCounter'] = range(lms_combined.shape[0])\n",
    "    lms_combined['sentence_index'] = i\n",
    "    sentence_store.append(lms_combined)\n",
    "wdf = pd.concat(sentence_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf['word'] = wdf[u'bnc_unigram_word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Deleted and Changed Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for generation n, get the indices of all words that have changed in n+1\n",
    "# have changed: no longer appear? doesn't handle transpositions\n",
    "# borrowed the function from the old version of telephone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/notebooks/telephone-analysis-public/lib/python3.6/site-packages/rpy2/robjects/pandas2ri.py:63: UserWarning: Error while trying to convert the column \"gold_candidate_transcription\". Fall back to string conversion. The error is: module 'pandas' has no attribute 'NA'\n",
      "  % (name, str(e)))\n",
      "/home/stephan/notebooks/telephone-analysis-public/lib/python3.6/site-packages/rpy2/robjects/pandas2ri.py:63: UserWarning: Error while trying to convert the column \"user_candidate_transcription\". Fall back to string conversion. The error is: module 'pandas' has no attribute 'NA'\n",
      "  % (name, str(e)))\n"
     ]
    }
   ],
   "source": [
    "# take a 2-column subset of all_trials that we can put into R\n",
    "input_output = all_runs[['gold_candidate_transcription','user_candidate_transcription']]\n",
    "#gold_candidate_transcription is what a participant heard\n",
    "#user_candidate_transcription is what the participant produced\n",
    "\n",
    "# remove the intitial sentences -- these are represnted as input for the first participant\n",
    "%R -i input_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"3193 sentences\"\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "source('getWordLevenshteinDistance.R')\n",
    "print(paste(nrow(input_output), 'sentences'))\n",
    "names(input_output) = c('input','output')  \n",
    "input_output$input = tolower(as.character(input_output$input))\n",
    "input_output$output = tolower(as.character(input_output$output))\n",
    "\n",
    "computeEditTable = function(s,r){\n",
    "    if (s == 'none'){\n",
    "        # this is an initial sentence, return NA\n",
    "        return(NA)\n",
    "    } else {\n",
    "        et = getReducedEditTable(s,r)\n",
    "        # for python compatibility, use 0-indices\n",
    "        et$sCounter = et$sCounter - 1\n",
    "        et$rCounter = et$rCounter - 1\n",
    "        return(et)\n",
    "    }\n",
    "}\n",
    "\n",
    "editTables = mapply(computeEditTable, input_output$input, input_output$output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   code      sWord      rWord sCounter rCounter\n",
      "1     M     before     before        0        0\n",
      "2     M        you        you        1        1\n",
      "3     M      leave      leave        2        2\n",
      "4     M       make       make        3        3\n",
      "5     M       sure       sure        4        4\n",
      "6     M        you        you        5        5\n",
      "11    S       turn     change        6        6\n",
      "9     M everything everything        7        7\n",
      "10    D        off       <NA>        8       NA\n",
      "                                             sentence\n",
      "1  before you leave make sure you turn everything off\n",
      "2  before you leave make sure you turn everything off\n",
      "3  before you leave make sure you turn everything off\n",
      "4  before you leave make sure you turn everything off\n",
      "5  before you leave make sure you turn everything off\n",
      "6  before you leave make sure you turn everything off\n",
      "11 before you leave make sure you turn everything off\n",
      "9  before you leave make sure you turn everything off\n",
      "10 before you leave make sure you turn everything off\n",
      "                                            response       sLeftSequence\n",
      "1  before you leave make sure you change everything                     \n",
      "2  before you leave make sure you change everything                     \n",
      "3  before you leave make sure you change everything     before you leave\n",
      "4  before you leave make sure you change everything       you leave make\n",
      "5  before you leave make sure you change everything      leave make sure\n",
      "6  before you leave make sure you change everything        make sure you\n",
      "11 before you leave make sure you change everything        sure you turn\n",
      "9  before you leave make sure you change everything  you turn everything\n",
      "10 before you leave make sure you change everything  turn everything off\n",
      "           rLeftSequence      sRightSequence        rRightSequence\n",
      "1                           before you leave      before you leave\n",
      "2                             you leave make        you leave make\n",
      "3       before you leave     leave make sure       leave make sure\n",
      "4         you leave make       make sure you         make sure you\n",
      "5        leave make sure       sure you turn       sure you change\n",
      "6          make sure you you turn everything you change everything\n",
      "11       sure you change turn everything off                      \n",
      "9  you change everything                                          \n",
      "10                                                                \n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "editTables[[3193]] #this should correspond to wdf[,3192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  code sWord rWord sCounter rCounter  sentence        response sLeftSequence\n",
      "1    M  this  this        0        0 this is a this is a mouse              \n",
      "2    M    is    is        1        1 this is a this is a mouse              \n",
      "3    M     a     a        2        2 this is a this is a mouse     this is a\n",
      "4    I  <NA> mouse       NA        3 this is a this is a mouse              \n",
      "  rLeftSequence sRightSequence rRightSequence\n",
      "1                    this is a      this is a\n",
      "2                                  is a mouse\n",
      "3     this is a                              \n",
      "4    is a mouse                              \n",
      "  code sWord rWord sCounter rCounter        sentence  response sLeftSequence\n",
      "1    M  this  this        0        0 this is a mouse this is a              \n",
      "2    M    is    is        1        1 this is a mouse this is a              \n",
      "3    M     a     a        2        2 this is a mouse this is a     this is a\n",
      "4    D mouse  <NA>        3       NA this is a mouse this is a    is a mouse\n",
      "  rLeftSequence sRightSequence rRightSequence\n",
      "1                    this is a      this is a\n",
      "2                   is a mouse               \n",
      "3     this is a                              \n",
      "4                                            \n",
      "  code sWord rWord sCounter rCounter        sentence        response\n",
      "1    M  this  this        0        0 this is a mouse this is a mouse\n",
      "2    M    is    is        1        1 this is a mouse this is a mouse\n",
      "3    M     a     a        2        2 this is a mouse this is a mouse\n",
      "4    M mouse mouse        3        3 this is a mouse this is a mouse\n",
      "  sLeftSequence rLeftSequence sRightSequence rRightSequence\n",
      "1                                  this is a      this is a\n",
      "2                                 is a mouse     is a mouse\n",
      "3     this is a     this is a                              \n",
      "4    is a mouse    is a mouse                              \n",
      "   code sWord rWord sCounter rCounter        sentence        response\n",
      "1     M  this  this        0        0 this is a mouse this is a house\n",
      "2     M    is    is        1        1 this is a mouse this is a house\n",
      "3     M     a     a        2        2 this is a mouse this is a house\n",
      "11    S mouse house        3        3 this is a mouse this is a house\n",
      "   sLeftSequence rLeftSequence sRightSequence rRightSequence\n",
      "1                                   this is a      this is a\n",
      "2                                  is a mouse     is a house\n",
      "3      this is a     this is a                              \n",
      "11    is a mouse    is a house                              \n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "print(computeEditTable('this is a', 'this is a mouse'))\n",
    "print(computeEditTable('this is a mouse', 'this is a'))\n",
    "print(computeEditTable('this is a mouse', 'this is a mouse'))\n",
    "print(computeEditTable('this is a mouse', 'this is a house'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "getDSMwrtInput = function(sentence_index, editTables){\n",
    "    et = editTables[[sentence_index]]\n",
    "    if (is.na(et)){\n",
    "        return(data.frame(sentence_index, sCounter=NA,code=NA)) # these are initial sentences\n",
    "    } else{\n",
    "        et = editTables[[sentence_index]]    \n",
    "        et$sentence_index = sentence_index\n",
    "        # here is where insertions are removed so that we can join back with wfds\n",
    "        return(subset(et, !is.na(sCounter))[,c('sentence_index','sCounter','code')])\n",
    "    }\n",
    "}\n",
    "\n",
    "DSMwrtInput = do.call('rbind', lapply(c(1:length(editTables)), function(i){\n",
    "    dsm = getDSMwrtInput(i, editTables)  \n",
    "    if (!is.na(dsm)){\n",
    "        dsm$sentence_index = dsm$sentence_index - 2 \n",
    "        # -1 because Python indexes from 0\n",
    "        # another -1 to bring the index of edits into alignment with the languag model results     \n",
    "    } \n",
    "    return(dsm)\n",
    "}))   \n",
    "\n",
    "#DSMwrtInput = subset(DSMwrtInput, !is.na(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sCounter</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA_character_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_index  sCounter           code\n",
       "1              -1.0       NaN  NA_character_\n",
       "110             0.0       0.0              M\n",
       "2               0.0       1.0              D\n",
       "5               0.0       2.0              M\n",
       "6               0.0       3.0              M\n",
       "7               0.0       4.0              M\n",
       "8               0.0       5.0              M\n",
       "9               0.0       6.0              M\n",
       "11              0.0       7.0              S\n",
       "12              0.0       8.0              M\n",
       "13              0.0       9.0              M\n",
       "111             1.0       0.0              M\n",
       "21              1.0       1.0              D\n",
       "3               1.0       2.0              D\n",
       "51              1.0       3.0              M"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rpy2.robjects import r, pandas2ri\n",
    "pandas2ri.activate()\n",
    "DSMwrtInput = r['DSMwrtInput']\n",
    "DSMwrtInput.iloc[0:15,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialSentenceIndices = np.array(DSMwrtInput[np.isnan(DSMwrtInput.sCounter)]['sentence_index'].tolist())\n",
    "wdf_initRemoved = wdf[(~wdf['sentence_index'].isin(initialSentenceIndices)) & (wdf['bnc_unigram_word'] != '</s>')]\n",
    "DSMwrtInput_nansRemoved = DSMwrtInput[~np.isnan(DSMwrtInput.sCounter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27290\n",
      "30298\n"
     ]
    }
   ],
   "source": [
    "print(DSMwrtInput_nansRemoved.shape[0])\n",
    "print(wdf_initRemoved.shape[0]) #still different, why?\n",
    "#assert(DSMwrtInput_nansRemoved.shape[0] == wdf_initRemoved.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bnc_unigram_index</th>\n",
       "      <th>bnc_unigram_prob</th>\n",
       "      <th>bnc_unigram_word</th>\n",
       "      <th>bnc_trigram_index</th>\n",
       "      <th>bnc_trigram_prob</th>\n",
       "      <th>bnc_trigram_word</th>\n",
       "      <th>roark_scores_prefix</th>\n",
       "      <th>roark_scores_word</th>\n",
       "      <th>roark_scores_prefix.1</th>\n",
       "      <th>roark_scores_srprsl</th>\n",
       "      <th>...</th>\n",
       "      <th>big_lm_scores_Unnamed: 0</th>\n",
       "      <th>big_lm_scores_prob</th>\n",
       "      <th>big_lm_scores_word</th>\n",
       "      <th>kenlm_scores_prob</th>\n",
       "      <th>kenlm_scores_preceding</th>\n",
       "      <th>kenlm_scores_unk</th>\n",
       "      <th>kenlm_scores_words</th>\n",
       "      <th>sCounter</th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.081292</td>\n",
       "      <td>before</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.037731</td>\n",
       "      <td>before</td>\n",
       "      <td>pfix:1</td>\n",
       "      <td>before</td>\n",
       "      <td>6.993</td>\n",
       "      <td>3.037021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.060103</td>\n",
       "      <td>Before</td>\n",
       "      <td>-3.105574</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>before</td>\n",
       "      <td>0</td>\n",
       "      <td>3192</td>\n",
       "      <td>before</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.186153</td>\n",
       "      <td>you</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.185921</td>\n",
       "      <td>you</td>\n",
       "      <td>pfix:2</td>\n",
       "      <td>you</td>\n",
       "      <td>13.198</td>\n",
       "      <td>2.694363</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.458716</td>\n",
       "      <td>you</td>\n",
       "      <td>-1.241865</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>you</td>\n",
       "      <td>1</td>\n",
       "      <td>3192</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.713525</td>\n",
       "      <td>leave</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.507196</td>\n",
       "      <td>leave</td>\n",
       "      <td>pfix:3</td>\n",
       "      <td>leave</td>\n",
       "      <td>20.940</td>\n",
       "      <td>3.362308</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.782638</td>\n",
       "      <td>leave</td>\n",
       "      <td>-1.473630</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>leave</td>\n",
       "      <td>2</td>\n",
       "      <td>3192</td>\n",
       "      <td>leave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.121925</td>\n",
       "      <td>make</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-4.336845</td>\n",
       "      <td>make</td>\n",
       "      <td>pfix:4</td>\n",
       "      <td>make</td>\n",
       "      <td>30.850</td>\n",
       "      <td>4.303858</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-4.497365</td>\n",
       "      <td>make</td>\n",
       "      <td>-4.662948</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>make</td>\n",
       "      <td>3</td>\n",
       "      <td>3192</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-3.643197</td>\n",
       "      <td>sure</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.549859</td>\n",
       "      <td>sure</td>\n",
       "      <td>pfix:5</td>\n",
       "      <td>sure</td>\n",
       "      <td>36.383</td>\n",
       "      <td>2.402951</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.128034</td>\n",
       "      <td>sure</td>\n",
       "      <td>-1.491675</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>sure</td>\n",
       "      <td>4</td>\n",
       "      <td>3192</td>\n",
       "      <td>sure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.186153</td>\n",
       "      <td>you</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.752118</td>\n",
       "      <td>you</td>\n",
       "      <td>pfix:6</td>\n",
       "      <td>you</td>\n",
       "      <td>43.568</td>\n",
       "      <td>3.120406</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.309889</td>\n",
       "      <td>you</td>\n",
       "      <td>-0.993335</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>you</td>\n",
       "      <td>5</td>\n",
       "      <td>3192</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>-3.512815</td>\n",
       "      <td>change</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-3.870051</td>\n",
       "      <td>change</td>\n",
       "      <td>pfix:7</td>\n",
       "      <td>change</td>\n",
       "      <td>54.143</td>\n",
       "      <td>4.593098</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-3.559230</td>\n",
       "      <td>change</td>\n",
       "      <td>-3.912920</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>change</td>\n",
       "      <td>6</td>\n",
       "      <td>3192</td>\n",
       "      <td>change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-3.763360</td>\n",
       "      <td>everything</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-3.264657</td>\n",
       "      <td>everything</td>\n",
       "      <td>pfix:8</td>\n",
       "      <td>everything</td>\n",
       "      <td>63.772</td>\n",
       "      <td>4.181822</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-3.270689</td>\n",
       "      <td>everything</td>\n",
       "      <td>-3.290390</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>everything</td>\n",
       "      <td>7</td>\n",
       "      <td>3192</td>\n",
       "      <td>everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.231726</td>\n",
       "      <td>&lt;/S&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>3192</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bnc_unigram_index  bnc_unigram_prob bnc_unigram_word  bnc_trigram_index  \\\n",
       "0                0.0         -3.081292           before                0.0   \n",
       "1                1.0         -2.186153              you                1.0   \n",
       "2                2.0         -3.713525            leave                2.0   \n",
       "3                3.0         -3.121925             make                3.0   \n",
       "4                4.0         -3.643197             sure                4.0   \n",
       "5                5.0         -2.186153              you                5.0   \n",
       "6                6.0         -3.512815           change                6.0   \n",
       "7                7.0         -3.763360       everything                7.0   \n",
       "9                NaN               NaN              NaN                NaN   \n",
       "\n",
       "   bnc_trigram_prob bnc_trigram_word roark_scores_prefix roark_scores_word  \\\n",
       "0         -3.037731           before              pfix:1            before   \n",
       "1         -1.185921              you              pfix:2               you   \n",
       "2         -1.507196            leave              pfix:3             leave   \n",
       "3         -4.336845             make              pfix:4              make   \n",
       "4         -1.549859             sure              pfix:5              sure   \n",
       "5         -0.752118              you              pfix:6               you   \n",
       "6         -3.870051           change              pfix:7            change   \n",
       "7         -3.264657       everything              pfix:8        everything   \n",
       "9               NaN              NaN                 NaN               NaN   \n",
       "\n",
       "   roark_scores_prefix.1  roark_scores_srprsl  ...  big_lm_scores_Unnamed: 0  \\\n",
       "0                  6.993             3.037021  ...                       0.0   \n",
       "1                 13.198             2.694363  ...                       1.0   \n",
       "2                 20.940             3.362308  ...                       2.0   \n",
       "3                 30.850             4.303858  ...                       3.0   \n",
       "4                 36.383             2.402951  ...                       4.0   \n",
       "5                 43.568             3.120406  ...                       5.0   \n",
       "6                 54.143             4.593098  ...                       6.0   \n",
       "7                 63.772             4.181822  ...                       7.0   \n",
       "9                    NaN                  NaN  ...                       9.0   \n",
       "\n",
       "   big_lm_scores_prob  big_lm_scores_word  kenlm_scores_prob  \\\n",
       "0           -3.060103              Before          -3.105574   \n",
       "1           -2.458716                 you          -1.241865   \n",
       "2           -2.782638               leave          -1.473630   \n",
       "3           -4.497365                make          -4.662948   \n",
       "4           -1.128034                sure          -1.491675   \n",
       "5           -0.309889                 you          -0.993335   \n",
       "6           -3.559230              change          -3.912920   \n",
       "7           -3.270689          everything          -3.290390   \n",
       "9           -1.231726                </S>                NaN   \n",
       "\n",
       "   kenlm_scores_preceding  kenlm_scores_unk  kenlm_scores_words  sCounter  \\\n",
       "0                     2.0             False              before         0   \n",
       "1                     3.0             False                 you         1   \n",
       "2                     4.0             False               leave         2   \n",
       "3                     1.0             False                make         3   \n",
       "4                     2.0             False                sure         4   \n",
       "5                     3.0             False                 you         5   \n",
       "6                     2.0             False              change         6   \n",
       "7                     2.0             False          everything         7   \n",
       "9                     NaN               NaN                 NaN         9   \n",
       "\n",
       "   sentence_index        word  \n",
       "0            3192      before  \n",
       "1            3192         you  \n",
       "2            3192       leave  \n",
       "3            3192        make  \n",
       "4            3192        sure  \n",
       "5            3192         you  \n",
       "6            3192      change  \n",
       "7            3192  everything  \n",
       "9            3192         NaN  \n",
       "\n",
       "[9 rows x 27 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdf_initRemoved[wdf_initRemoved.sentence_index == 3192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sCounter</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sentence_index, sCounter, code]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DSMwrtInput_nansRemoved[DSMwrtInput_nansRemoved.sentence_index == 3192] \n",
    "#merging on sentence_index merges with the downstream!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>wdf_count</th>\n",
       "      <th>dsm_count</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>2141</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>2131</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>2132</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>2133</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>2134</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>2135</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>2136</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>2138</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>2139</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>2140</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>2142</td>\n",
       "      <td>12</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>2153</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>2143</td>\n",
       "      <td>12</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>2144</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>2145</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>2146</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>2147</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>2148</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>2149</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>2150</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>2151</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>2130</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>2129</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>2128</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>2126</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>2105</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>2106</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>2107</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>2108</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>1092</td>\n",
       "      <td>8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>1093</td>\n",
       "      <td>8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>1094</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>1096</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>1097</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>1077</td>\n",
       "      <td>8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>1075</td>\n",
       "      <td>8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>1051</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>1074</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>1052</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>1053</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>1055</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>1056</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>1057</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>1058</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>1059</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>1060</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1061</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1062</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1063</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1064</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1065</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1066</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1068</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>1069</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>1071</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>1072</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>1073</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>3191</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>3192</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence_index  wdf_count  dsm_count  difference\n",
       "0                  0         11       10.0         1.0\n",
       "2003            2141         11       10.0         1.0\n",
       "1994            2131         10        9.0         1.0\n",
       "1995            2132         10        9.0         1.0\n",
       "1996            2133         10        9.0         1.0\n",
       "1997            2134         10        9.0         1.0\n",
       "1998            2135         10        9.0         1.0\n",
       "1999            2136         11       10.0         1.0\n",
       "2000            2138         11       10.0         1.0\n",
       "2001            2139         11       10.0         1.0\n",
       "2002            2140         11       10.0         1.0\n",
       "2004            2142         12       11.0         1.0\n",
       "2015            2153          9        8.0         1.0\n",
       "2005            2143         12       11.0         1.0\n",
       "2006            2144         11       10.0         1.0\n",
       "2007            2145         11       10.0         1.0\n",
       "2008            2146         11       10.0         1.0\n",
       "2009            2147         10        9.0         1.0\n",
       "2010            2148         10        9.0         1.0\n",
       "2011            2149         10        9.0         1.0\n",
       "2012            2150         10        9.0         1.0\n",
       "2013            2151         10        9.0         1.0\n",
       "1993            2130         10        9.0         1.0\n",
       "1992            2129         11       10.0         1.0\n",
       "1991            2128         11       10.0         1.0\n",
       "1990            2126         11       10.0         1.0\n",
       "1971            2105         11       10.0         1.0\n",
       "1972            2106         11       10.0         1.0\n",
       "1973            2107         11       10.0         1.0\n",
       "1974            2108         11       10.0         1.0\n",
       "...              ...        ...        ...         ...\n",
       "1024            1092          8        7.0         1.0\n",
       "1025            1093          8        7.0         1.0\n",
       "1026            1094          9        8.0         1.0\n",
       "1027            1096         11       10.0         1.0\n",
       "1028            1097         11       10.0         1.0\n",
       "1009            1077          8        7.0         1.0\n",
       "1007            1075          8        7.0         1.0\n",
       "986             1051         11       10.0         1.0\n",
       "1006            1074          9        8.0         1.0\n",
       "987             1052         11       10.0         1.0\n",
       "988             1053         11       10.0         1.0\n",
       "989             1055         11       10.0         1.0\n",
       "990             1056         11       10.0         1.0\n",
       "991             1057         11       10.0         1.0\n",
       "992             1058         11       10.0         1.0\n",
       "993             1059         11       10.0         1.0\n",
       "994             1060         11       10.0         1.0\n",
       "995             1061         11       10.0         1.0\n",
       "996             1062         11       10.0         1.0\n",
       "997             1063         11       10.0         1.0\n",
       "998             1064         11       10.0         1.0\n",
       "999             1065         11       10.0         1.0\n",
       "1000            1066         10        9.0         1.0\n",
       "1001            1068         11       10.0         1.0\n",
       "1002            1069         10        9.0         1.0\n",
       "1003            1071         11       10.0         1.0\n",
       "1004            1072         10        9.0         1.0\n",
       "1005            1073          9        8.0         1.0\n",
       "2998            3191         10        9.0         1.0\n",
       "2999            3192          9        NaN         NaN\n",
       "\n",
       "[3000 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsm_counts = DSMwrtInput_nansRemoved.groupby(['sentence_index']).sCounter.agg(np.size).reset_index()\n",
    "\n",
    "dsm_counts.columns = ['sentence_index', 'dsm_count']\n",
    "wdf_counts = wdf_initRemoved.groupby('sentence_index').sCounter.agg(np.size).reset_index()\n",
    "wdf_counts.columns = ['sentence_index', 'wdf_count']\n",
    "count_check = wdf_counts.merge(dsm_counts, how='outer')\n",
    "count_check # differences in the counts?\n",
    "count_check['difference'] = count_check.wdf_count - count_check.dsm_count\n",
    "count_check.sort_values(by=['difference'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge DSMwrtInput into the word data frame\n",
    "wdfr = wdf_initRemoved.merge(DSMwrtInput_nansRemoved) #sCounter is NaN for input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    22482\n",
       "D     3442\n",
       "S     1366\n",
       "Name: code, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#where did all my happy data go?\n",
    "wdfr.code.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge With Word Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before merging with Lexiconch: 27290\n",
      "Number of words after merging with Lexiconch: 27290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/notebooks/telephone-analysis-public/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (9,10,12,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdfr['word'] = wdfr['bnc_unigram_word']\n",
    "lexiconch = pd.read_csv('data/lexiconch.csv', index_col=0)\n",
    "print('Number of words before merging with Lexiconch: '+str(wdfr.shape[0]))\n",
    "wdfl = wdfr.merge(lexiconch, how='left')\n",
    "print('Number of words after merging with Lexiconch: '+str(wdfl.shape[0]))\n",
    "' '.join(set(wdfr.word) - set(wdfl.word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word                               roadsweeper\n",
       "conc_bigram                                  0\n",
       "conc_conc_m                               4.85\n",
       "conc_conc_sd                              0.37\n",
       "conc_unknown                                 1\n",
       "conc_total                                  27\n",
       "conc_percent_known                        0.96\n",
       "conc_subtlex                                 0\n",
       "conc_dom_pos                                 0\n",
       "kpm_alternative_spelling          road sweeper\n",
       "kpm_freq_pm                                NaN\n",
       "kpm_dom_pos_subtlex                        NaN\n",
       "kpm_nletters                                11\n",
       "kpm_nphon                                    8\n",
       "kpm_nsyll                                    3\n",
       "kpm_lemma_highest_pos              roadsweeper\n",
       "kpm_aoa_kup                               9.83\n",
       "kpm_perc_known                               1\n",
       "kpm_aoa_kup_lem                           9.83\n",
       "kpm_perc_known_lem                           1\n",
       "kpm_aoa_bird_lem                          7.12\n",
       "kpm_aoa_bristol_lem                        NaN\n",
       "kpm_aoa_cort_lem                           NaN\n",
       "kpm_aoa_schock                             NaN\n",
       "pic_ipa                                    NaN\n",
       "pic_ipa_ss_array                           NaN\n",
       "pic_ipa_ss                                 NaN\n",
       "pic_ipa_n                                  NaN\n",
       "pic_ortho                                  NaN\n",
       "pic_ortho_ss_array                         NaN\n",
       "pic_ortho_ss                               NaN\n",
       "pic_ortho_n                                NaN\n",
       "pic_character                              NaN\n",
       "pic_character_ss_array                     NaN\n",
       "pic_character_ss                           NaN\n",
       "pic_character_n                            NaN\n",
       "lic_mean_surprisal_weighted                NaN\n",
       "lic_mean_surprisal_unweighted              NaN\n",
       "lic_frequency                              NaN\n",
       "lic_numcontexts                            NaN\n",
       "lic_retrievaltime                          NaN\n",
       "childes_parentProb                         NaN\n",
       "childes_parentUnigramSurprisal             NaN\n",
       "childes_childProb                          NaN\n",
       "childes_childUnigramSurprisal              NaN\n",
       "childes_childEarliestUse                   NaN\n",
       "wordbank_median_aoa                        NaN\n",
       "propuse_childes_24                           0\n",
       "wordbank_prop_use                          NaN\n",
       "propuse_combined                             0\n",
       "in_mcdi                                     -2\n",
       "wn_lemma                           roadsweeper\n",
       "aspell_inDictionary                          0\n",
       "has_apostrophe                               0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexiconch.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/notebooks/telephone-analysis-public/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "yarkoni_pld = pd.read_table('data/pld20.txt', header=None)\n",
    "yarkoni_pld.columns = ['word', 'pld20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before merging with Yarkoni PLD: 27290\n",
      "Number of words after merging with Yarkoni PLD: 27290\n"
     ]
    }
   ],
   "source": [
    "print('Number of words before merging with Yarkoni PLD: '+str(wdfl.shape[0]))\n",
    "wdfy = wdfl.merge(yarkoni_pld, how='left')\n",
    "print('Number of words after merging with Yarkoni PLD: '+str(wdfy.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word                         the\n",
       "FREQcount                1501908\n",
       "CDcount                     8388\n",
       "FREQlow                  1339811\n",
       "Cdlow                       8388\n",
       "SUBTLWF                  29449.2\n",
       "Lg10WF                    6.1766\n",
       "SUBTLCD                      100\n",
       "Lg10CD                    3.9237\n",
       "ipa_                      [Ã°, É™]\n",
       "ipa_single                [Ã°, É™]\n",
       "ipa_diphthongs            [Ã°, É™]\n",
       "unigram_probability    0.0302076\n",
       "unigram_surprisal        3.49966\n",
       "word                         the\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtlex = pd.read_csv('data/subtlex_augmented.csv')\n",
    "subtlex['word'] = subtlex.Word\n",
    "subtlex.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before merging with Subtlex: 27290\n",
      "Number of words after merging with Subtlex: 27290\n"
     ]
    }
   ],
   "source": [
    "print('Number of words before merging with Subtlex: '+str(wdfy.shape[0]))\n",
    "wdfx = wdfy.merge(subtlex[['word','SUBTLCD']], how='left')\n",
    "print('Number of words after merging with Subtlex: '+str(wdfx.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge in Sentence Level Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs['sentence_index'] = range(all_runs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdfs = wdfx.merge(all_runs, on='sentence_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bnc_unigram_index bnc_unigram_prob bnc_unigram_word bnc_trigram_index bnc_trigram_prob bnc_trigram_word roark_scores_prefix roark_scores_word roark_scores_prefix.1 roark_scores_srprsl roark_scores_SynSp roark_scores_LexSp roark_scores_ambig roark_scores_open roark_scores_rernk roark_scores_toprr roark_scores_stps big_lm_scores_Unnamed: 0 big_lm_scores_prob big_lm_scores_word kenlm_scores_prob kenlm_scores_preceding kenlm_scores_unk kenlm_scores_words sCounter sentence_index word code conc_bigram conc_conc_m conc_conc_sd conc_unknown conc_total conc_percent_known conc_subtlex conc_dom_pos kpm_alternative_spelling kpm_freq_pm kpm_dom_pos_subtlex kpm_nletters kpm_nphon kpm_nsyll kpm_lemma_highest_pos kpm_aoa_kup kpm_perc_known kpm_aoa_kup_lem kpm_perc_known_lem kpm_aoa_bird_lem kpm_aoa_bristol_lem kpm_aoa_cort_lem kpm_aoa_schock pic_ipa pic_ipa_ss_array pic_ipa_ss pic_ipa_n pic_ortho pic_ortho_ss_array pic_ortho_ss pic_ortho_n pic_character pic_character_ss_array pic_character_ss pic_character_n lic_mean_surprisal_weighted lic_mean_surprisal_unweighted lic_frequency lic_numcontexts lic_retrievaltime childes_parentProb childes_parentUnigramSurprisal childes_childProb childes_childUnigramSurprisal childes_childEarliestUse wordbank_median_aoa propuse_childes_24 wordbank_prop_use propuse_combined in_mcdi wn_lemma aspell_inDictionary has_apostrophe pld20 SUBTLCD Unnamed: 0 chain character_levdau check_time condition flag_type gold_candidate_transcription gold_comparison_transcription gold_dist length_accept reason run stimulus stimulus_id subject_id target_length upload_time upstream_pointer upstream_subject_id user user_candidate_transcription user_comparison_transcription user_dist user_short word_distances unique_chain_identifier global_chain BNC_KNN_trigramProb BNC_KNN_unigramProb WSJ_gt_unigramProb WSJ_gt_trigramProb WSJ_gt_5gramProb WSJ_Roark_Negative.Log.Probability biglm_probability kenlm_probability bllip_probability bllip_wsj_probability mikolov_wsj_probability length_in_words normalized_biglm_probability normalized_WSJ_Roark_Negative.Log.Probability normalized_BNC_KNN_unigramProb normalized_BNC_KNN_trigramProb normalized_kenlm_probability normalized_bllip_probability normalized_bllip_wsj_probability normalized_mikolov_wsj_probability normalized_WSJ_gt_unigramProb normalized_WSJ_gt_trigramProb normalized_WSJ_gt_5gramProb initial_biglm_probability initial_biglm_probability_rank initial_biglm_probability_quartile initial_WSJ_Roark_Negative.Log.Probability initial_WSJ_Roark_Negative.Log.Probability_rank initial_WSJ_Roark_Negative.Log.Probability_quartile initial_BNC_KNN_unigramProb initial_BNC_KNN_unigramProb_rank initial_BNC_KNN_unigramProb_quartile initial_BNC_KNN_trigramProb initial_BNC_KNN_trigramProb_rank initial_BNC_KNN_trigramProb_quartile initial_kenlm_probability initial_kenlm_probability_rank initial_kenlm_probability_quartile initial_bllip_probability initial_bllip_probability_rank initial_bllip_probability_quartile initial_bllip_wsj_probability initial_bllip_wsj_probability_rank initial_bllip_wsj_probability_quartile initial_mikolov_wsj_probability initial_mikolov_wsj_probability_rank initial_mikolov_wsj_probability_quartile initial_WSJ_gt_unigramProb initial_WSJ_gt_unigramProb_rank initial_WSJ_gt_unigramProb_quartile initial_WSJ_gt_trigramProb initial_WSJ_gt_trigramProb_rank initial_WSJ_gt_trigramProb_quartile initial_WSJ_gt_5gramProb initial_WSJ_gt_5gramProb_rank initial_WSJ_gt_5gramProb_quartile thread_id chain_length'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(wdfs.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27290, 168)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdfs.to_csv('output/wordLevelChanges.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See R notebook, Error Prediction Model - Logistic Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telephone3",
   "language": "python",
   "name": "telephone3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
